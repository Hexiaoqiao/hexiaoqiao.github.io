
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>HDFS升级过程中重启方式选择 - Hexiaoqiao</title>
  <meta name="author" content="Hexiaoqiao">

  
  <meta name="description" content="一、背景 集群在运行过程中，由于升级等原因难免会遇到重启NameNode或整个集群节点的情况，不同的重启方式会影响到整个运维操作的效率。
刚接触HDFS时的某次全集群内DataNode升级，遇到一次非预期内的NameNode重启。升级时，NameNode首先进入Safemode模式， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://hexiaoqiao.github.io/blog/2016/07/07/restart-cluster-ways-when-upgrade/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Hexiaoqiao" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-72478952-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Hexiaoqiao</a></h1>
  
    <h2>Focus on BigData,Distributed System,Hadoop 2.*</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="hexiaoqiao.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">HDFS升级过程中重启方式选择</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-07-07T10:45:00+08:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>7</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>10:45 am</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><h2>一、背景</h2>

<p>集群在运行过程中，由于升级等原因难免会遇到重启NameNode或整个集群节点的情况，不同的重启方式会影响到整个运维操作的效率。<br/>
刚接触HDFS时的某次全集群内DataNode升级，遇到一次非预期内的NameNode重启。升级时，NameNode首先进入Safemode模式，全集群禁止写操作。DataNode数据包和配置更新后操作了所有DataNode一次性重启，之后NameNode间歇性不能响应，持续高负载达~45min，之后不得不通过重启NameNode，之后~35min全集群启动完成，服务恢复正常。</p>

<p>由此引出问题：<br/>
1、造成全集群DataNode重启后NameNode不能正常响应的根本原因是什么？<br/>
2、重启NameNode为什么能够实现恢复服务？</p>

<h2>二、原因分析</h2>

<h3>1、现场还原</h3>

<p>在集群重启过程中，不管以什么方式进行重启，避免不了DataNode向NameNode进行BlockReport的交互，从NameNode现场截取两个时间段里部分BlockReport日志。</p>

<p>表1 不同重启方式NameNode处理BR时间统计</p>

<table>
<thead>
<tr>
<th style="text-align:center;">  <strong>DN Restart Only</strong>   </th>
<th style="text-align:center;"> - </th>
<th style="text-align:center;"> - </th>
<th style="text-align:center;"> <strong>With NN Restart</strong> </th>
<th style="text-align:center;"> - </th>
<th style="text-align:center;"> - </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> <strong>Date</strong> </td>
<td style="text-align:center;"> <strong>Blocks</strong> </td>
<td style="text-align:center;"> <strong>ProcessTime</strong> </td>
<td style="text-align:center;"> <strong>Date</strong> </td>
<td style="text-align:center;"> <strong>Blocks</strong> </td>
<td style="text-align:center;"> <strong>ProcessTime</strong> </td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:51</td>
<td style="text-align:center;">   49428</td>
<td style="text-align:center;">  646 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   66392</td>
<td style="text-align:center;">  73 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:51</td>
<td style="text-align:center;">   49428</td>
<td style="text-align:center;">  646 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   66392</td>
<td style="text-align:center;">  73 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:51</td>
<td style="text-align:center;">   49068</td>
<td style="text-align:center;">  644 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   69978</td>
<td style="text-align:center;">  84 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:52</td>
<td style="text-align:center;">   51822</td>
<td style="text-align:center;">  638 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   78222</td>
<td style="text-align:center;">  98 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:53</td>
<td style="text-align:center;">   83131</td>
<td style="text-align:center;">  1214 msecs      </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   64663</td>
<td style="text-align:center;">  71 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:53</td>
<td style="text-align:center;">   90088</td>
<td style="text-align:center;">  169 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   85106</td>
<td style="text-align:center;">  99 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:54</td>
<td style="text-align:center;">   82024</td>
<td style="text-align:center;">  1107 msecs      </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   87346</td>
<td style="text-align:center;">  96 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:55</td>
<td style="text-align:center;">   48114</td>
<td style="text-align:center;">  637 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   87802</td>
<td style="text-align:center;">  96 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:55</td>
<td style="text-align:center;">   49457</td>
<td style="text-align:center;">  84 msecs        </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   65646</td>
<td style="text-align:center;">  71 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:56</td>
<td style="text-align:center;">   82989</td>
<td style="text-align:center;">  457 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   71025</td>
<td style="text-align:center;">  86 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:57</td>
<td style="text-align:center;">   84634</td>
<td style="text-align:center;">  1181 msecs      </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   66144</td>
<td style="text-align:center;">  73 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:58</td>
<td style="text-align:center;">   67321</td>
<td style="text-align:center;">  885 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   72652</td>
<td style="text-align:center;">  90 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:59</td>
<td style="text-align:center;">   70668</td>
<td style="text-align:center;">  924 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:57</td>
<td style="text-align:center;">   66118</td>
<td style="text-align:center;">  76 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:59</td>
<td style="text-align:center;">   73114</td>
<td style="text-align:center;">  138 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   67011</td>
<td style="text-align:center;">  74 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:15:59</td>
<td style="text-align:center;">   28215</td>
<td style="text-align:center;">  692 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   78216</td>
<td style="text-align:center;">  84 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:00</td>
<td style="text-align:center;">   30080</td>
<td style="text-align:center;">  321 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   60988</td>
<td style="text-align:center;">  66 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:00</td>
<td style="text-align:center;">   30435</td>
<td style="text-align:center;">  329 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   52376</td>
<td style="text-align:center;">  58 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:00</td>
<td style="text-align:center;">   34350</td>
<td style="text-align:center;">  360 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   66801</td>
<td style="text-align:center;">  73 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:01</td>
<td style="text-align:center;">   32487</td>
<td style="text-align:center;">  344 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   49134</td>
<td style="text-align:center;">  53 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:01</td>
<td style="text-align:center;">   28244</td>
<td style="text-align:center;">  308 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   66928</td>
<td style="text-align:center;">  73 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:01</td>
<td style="text-align:center;">   29138</td>
<td style="text-align:center;">  308 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   75560</td>
<td style="text-align:center;">  82 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:02</td>
<td style="text-align:center;">   29765</td>
<td style="text-align:center;">  301 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   83880</td>
<td style="text-align:center;">  92 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:02</td>
<td style="text-align:center;">   28699</td>
<td style="text-align:center;">  309 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   82989</td>
<td style="text-align:center;">  93 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:02</td>
<td style="text-align:center;">   35377</td>
<td style="text-align:center;">  370 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   56210</td>
<td style="text-align:center;">  60 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:03</td>
<td style="text-align:center;">   49204</td>
<td style="text-align:center;">  626 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   65517</td>
<td style="text-align:center;">  78 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:03</td>
<td style="text-align:center;">   27554</td>
<td style="text-align:center;">  438 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   76159</td>
<td style="text-align:center;">  78 msecs</td>
</tr>
<tr>
<td style="text-align:center;">2015-04-28 13:16:04</td>
<td style="text-align:center;">   27285</td>
<td style="text-align:center;">  326 msecs       </td>
<td style="text-align:center;">2015-04-28 14:00:58</td>
<td style="text-align:center;">   59725</td>
<td style="text-align:center;">  58 msecs</td>
</tr>
</tbody>
</table>


<p>左半部分记录的是重启全集群DataNode后，NameNode处理单个BlockReport请求耗时，右半部分为重启NameNode后，处理单个BlockReport请求耗时。这里只列了部分数据，虽不具统计意义，但是在处理时间的量级上可信。</p>

<p>从数据上可以看到，对于BlockReport类型的RPC请求，不同的重启方式，RPC的处理时间有明显差异。</p>

<h3>2、深度分析</h3>

<p>前面也提到从数据上看，对于BlockReport类型的RPC请求，重启全集群DataNode与重启NameNode，RPC处理时间有一个数量级的差别。这种差别通过代码得到验证。</p>

<figure class='code'><figcaption><span>BlockManager.java</span><a href='https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1813'>Github </a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="k">if</span> <span class="o">(</span><span class="n">storageInfo</span><span class="o">.</span><span class="na">getBlockReportCount</span><span class="o">()</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// The first block report can be processed a lot more efficiently than</span>
</span><span class='line'>  <span class="c1">// ordinary block reports.  This shortens restart times.</span>
</span><span class='line'>  <span class="n">processFirstBlockReport</span><span class="o">(</span><span class="n">node</span><span class="o">,</span> <span class="n">storage</span><span class="o">.</span><span class="na">getStorageID</span><span class="o">(),</span> <span class="n">newReport</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">invalidatedBlocks</span> <span class="o">=</span> <span class="n">processReport</span><span class="o">(</span><span class="n">node</span><span class="o">,</span> <span class="n">storage</span><span class="o">,</span> <span class="n">newReport</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>可以看到NameNode对BlockReport的处理方式仅区别于是否为初次BlockReport。初次BlockReport显然只发生在NameNode重启期间。<br/>
processFirstBlockReport：对Standby节点（NameNode重启期间均为Standby），如果汇报的数据块相关元数据还没有加载，会将报告的块信息暂存队列，当Standby节点完成加载相关元数据后，再处理该消息队列； 对第一次块汇报的处理比较特别，为提高处理效率，仅验证块是否损坏，然后判断块状态是否为FINALIZED状态，如果是建立块与DN节点的映射，其他信息一概暂不处理。<br/>
processReport：对于非初次块汇报，处理逻辑要复杂很多；对报告的每个块信息，不仅会建立块与DN的映射，还会检查是否损坏，是否无效，是否需要删除，是否为UC状态等等。</p>

<p>初次块汇报的处理逻辑单独拿出来，主要原因有两方面：<br/>
1、加快NameNode的启动时间；统计数据也能说明，初次块汇报的处理时间比正常块汇报的处理时间能节省约一个数量级的时间。<br/>
2、由于启动过程中，不提供正常读写服务，所以只要确保正常数据（整个Namespace和所有FINALIZED状态Blocks）无误，无效和冗余数据处理完全可以延后到IBR或next BR。<br/>
说明：<br/>
1、是否选择processFirstBlockReport处理逻辑不会因为NameNode当前为safemode或者standby发生变化，仅NameNode重启生效；<br/>
2、BlockReport的处理时间与DataNode数据规模正相关，当前DataNode中Block数处于：200,000 ~ 1,000,000。<br/>
如果不操作NameNode重启，BlockReport处理时间会因为处理逻辑复杂带来额外的处理时间，统计数据显示，约一个数量级的差别。</p>

<p>NameNode对非第一次BlockReport的复杂处理逻辑只是NameNode负载持续处于高位的诱因，在其诱发下发生了一系列“滚雪球”式的异常放大。<br/>
1、所有DataNode进程被关闭后，NameNode的CallQueue（默认大小：3200）会被快速消费完；</p>

<div class=“pic” align="center" padding=“0”>
<img src="/images/restart/1.png" align="center"><br />
</div>


<p>2、所有DataNode进程被重启后，NameNode的CallQueue会被迅速填充，主要来自DataNode重启后正常流程里的VersionRequest和registerDataNode两类RPC请求，由于均较轻量，所以也会被迅速消费完；</p>

<div class=“pic” align="center" padding=“0”>
<img src="/images/restart/2.png" align="center"><br />
</div>


<p>3、之后DataNode进入BlockReport流程，NameNode的CallQueue填充内容开始从VersionRequest和registerDataNode向BlockReport过渡；</p>

<div class=“pic” align="center" padding=“0”>
<img src="/images/restart/3.png" align="center"><br />
</div>


<p>直到CallQueue里几乎被所有BlockReport填充满。</p>

<div class=“pic” align="center" padding=“0”>
<img src="/images/restart/4.png" align="center"><br />
</div>


<p>前面的统计数据显示，NameNode不重启对BlockReport的处理时间~500ms，另一个关键数据是Client看到的RPC超时时间，默认为60s；在默认的RPC超时时间范围内，CallQueue里最多可能被处理的BlockReport数~120个，其它均会发生超时。
当发生超时后，Client端（DataNode）会尝试重试，所以NameNode的CallQueue会被持续打满；另一方面，如果NameNode发现RPC Request出现超时会被忽略（可以从日志证实），直到存在未超时的请求，此时从CallQueue拿出来的BlockReport请求虽未超时，但也处于即将超时的边缘，即使处理完成其中的少数几个，CallQueue中的剩余大部分也会出现超时。</p>

<figure class='code'><figcaption><span>namenode.lgo </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>2015-04-28 13:14:48,709 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.45.38:37649 Call#650 Retry#0
</span><span class='line'>2015-04-28 13:14:48,709 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.53.5:14839 Call#659 Retry#0
</span><span class='line'>2015-04-28 13:14:48,709 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.62.5:55833 Call#702 Retry#0
</span><span class='line'>2015-04-28 13:14:48,709 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.51.31:41016 Call#655 Retry#0
</span><span class='line'>2015-04-28 13:14:48,709 INFO org.apache.hadoop.ipc.Server: IPC Server handler 29 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.62.36:53163 Call#702 Retry#0
</span><span class='line'>2015-04-28 13:14:48,709 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.46.32:53530 Call#662 Retry#0
</span><span class='line'>2015-04-28 13:14:48,710 INFO org.apache.hadoop.ipc.Server: IPC Server handler 29 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.55.11:52372 Call#662 Retry#0
</span><span class='line'>2015-04-28 13:14:48,710 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.55.44:30295 Call#666 Retry#0
</span><span class='line'>2015-04-28 13:14:48,710 INFO org.apache.hadoop.ipc.Server: IPC Server handler 29 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.16.50.4:37880 Call#674 Retry#0
</span></code></pre></td></tr></table></div></figure>


<p>通过前面的分析，从整个Timeline上看，NameNode长期处于满负荷运行状态，但是有效处理能力非常低（仅针对BlockReport）。这也是为什么1000+ DataNode（每一个DataNode管理的Block数均未超过1,000,000），也即1000+有效BlockReport请求，在~50min内依然没有被处理完成。</p>

<p>如果DataNode进程处于正常运行状态下，重启NameNode后会发生完全不同的情况。<br/>
1、NameNode重启后，首先加载FsImage，此时，除Namespace外NameNode的元数据几乎为空，此后开始接收DataNode过来的RPC请求（绝大多数为Heartbeat）；</p>

<div class=“pic” align="center" padding=“0”> 
<img src="/images/restart/5.png" align="center"><br />
</div>


<p>2、NameNode接收到Heartbeat后由于在初始状态会要求DataNode重新注册；由于Heartbeat间隔是3s，所以从NameNode的角度看，所有DataNode的后续一系列RPC请求会被散列到3s时间线上；</p>

<div class=“pic” align="center" padding=“0”>  
<img src="/images/restart/6.png" align="center"><br />
</div>


<p>3、DataNode向NameNode注册完成后立即开始BlockReport；由于步骤2里提到的3s时间线散列关系，队列里后半部分BlockReport请求和VersionRequest/registerDataNode请求会出现相互交叉的情况；</p>

<div class=“pic” align="center" padding=“0”>
<img src="/images/restart/7.png" align="center"><br />
</div>


<p>4、如前述，处理BlockReport时部分RPC请求一样会发生超时；</p>

<div class=“pic” align="center" padding=“0”>
<img src="/images/restart/8.png" align="center"><br />
</div>


<p>5、由于超时重试，所以部分BlockReport和registerDataNode需要重试；可以发现不同于重启所有DataNode时重试的RPC几乎都是BlockReport，这里重试的RPC包括了VersionRequest/registerDataNode（可以从日志证实），这就大幅降低了NameNode的负载，避免了“滚雪球”式高负载RPC堆积，使异常有效收敛。</p>

<figure class='code'><figcaption><span>namenode.log </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.32.73.39:16329 Call#2893 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.32.20.15:54831 Call#2889 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.16.62.38:10818 Call#2835 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.16.52.18:59462 Call#2818 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.registerDatanode from 10.16.39.24:13728 Call#2864 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.32.27.8:58789 Call#2883 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.registerDatanode from 10.32.73.40:56606 Call#2889 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.16.40.21:19961 Call#2843 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.16.43.13:22644 Call#2870 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.registerDatanode from 10.16.43.26:16289 Call#2876 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.16.61.30:31968 Call#2825 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.32.21.5:47752 Call#2879 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport from 10.32.49.11:46892 Call#2904 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.16.36.24:12326 Call#2859 Retry#0
</span><span class='line'>2015-04-28 14:01:19,302 INFO org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8020: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 10.16.56.4:55321 Call#2833 Retry#0
</span></code></pre></td></tr></table></div></figure>


<h3>3、避免重启大量DataNode时雪崩</h3>

<p>从前面的分析过程，可以得出两个结论：<br/>
（1）NameNode对正常BlockReport处理效率是造成可能雪崩的根本原因；<br/>
（2）BlockReport的堆积让问题完全失控；</p>

<p>从这两个结论出发可以推导出相应的解决办法：</p>

<p>1、解决效率问题：<br/>
（1）优化代码逻辑；这块代码相对成熟，可优化的空间不大，另外所需的时间成本较高，暂可不考虑；<br/>
（2）降低BlockReport时数据规模；NameNode处理BR的效率低主要原因还是每次BR所带的Block规模过大造成，所以可以通过调整Block数量阈值，将一次BlockReport分成多盘分别汇报，提高NameNode处理效率。可参考的参数为：dfs.blockreport.split.threshold，默认为1,000,000，当前集群DataNode上Block规模数处于240,000 ~ 940,000，建议调整为500,000；另一方面，可以通过在同一个物理节点上部署多个DataNode实例，分散数据，达到缩小规模的目的，但是这种方案仅能解决当前问题，长期来看依然不能避免，且影响范围比较大，需要多方面权衡。</p>

<p>2、解决堆积问题：<br/>
（1）控制重启DataNode的数量；按照当前节点数据规模，如果大规模重启DataNode，可采取滚动方式，以~15/单位间隔~1min滚动重启，如果数据规模增长，需要适当调整实例个数；<br/>
（2）定期清空CallQueue；如前述，当大规模DataNode实例被同时重启后，如果不采取措施一定会发生“雪崩”，若确实存在类似需求或场景，可以通过定期清空CallQueue（dfsadmin -refreshCallQueue）的方式，避免堆积效应；这种方案的弊端在于不能有选择的清空RPC Request，所以当线上服务期时，存在数据读写请求超时、作业失败的风险。</p>

<p>3、选择合适的重启方式：<br/>
（1）当需要对全集群的DataNode重启操作，且规模较大（包括集群规模和数据规模）时，建议在重启DataNode进程之后将NameNode重启，避免前面的“雪崩”问题；<br/>
（2）当灰度操作部分DataNode或者集群规模和数据规模均较小时，可采取滚动重启DataNode进程的方式；</p>

<h2>三、总结</h2>

<p>1、重启所有DataNode时，由于处理BlockReport逻辑不同，及由此诱发的“雪崩式”效应，导致重启进度极度缓慢；<br/>
2、在数据规模达到10K~100K，重启一台DataNode都会给NameNode的正常服务造成瞬时抖动；<br/>
3、在数据规模到100K量级时，同时重启~15以内DataNode不会对集群造成雪崩式灾难，但是可能出现短时间内服务不可用状态；<br/>
4、全集群升级时，建议NameNode和DataNode均重启，在预期时间内可恢复服务。</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">hexiaoqiao</span></span>

      




<time class='entry-date' datetime='2016-07-07T10:45:00+08:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>7</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>10:45 am</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/hdfs/'>hdfs</a>, <a class='category' href='/blog/categories/namenode/'>namenode</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://hexiaoqiao.github.io/blog/2016/07/07/restart-cluster-ways-when-upgrade/" data-via="" data-counturl="http://hexiaoqiao.github.io/blog/2016/07/07/restart-cluster-ways-when-upgrade/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/07/06/namenode-memory-overview/" title="Previous Post: NameNode内存全景">&laquo; NameNode内存全景</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/07/21/namenode-memory-detail/" title="Next Post: NameNode内存详解">NameNode内存详解 &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2019/04/26/discussion-on-the-optimization-of-hdfs-global-lock-mechanism/">HDFS锁机制优化方向讨论</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/10/05/recruit/">大数据职位招聘</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/07/13/a-brief-introduction-of-hdfs-blocktoken-mechanism/">HDFS BlockToken机制解析</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/03/30/the-analysis-of-basic-principle-of-hdfs-ha-using-qjm/">HDFS HA Using QJM原理解析</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/02/12/namenode-restart-optimization/">HDFS NameNode重启优化</a>
      </li>
    
  </ul>
</section>
  
<section>  
  <h1>Weibo</h1>  
  <ul id="weibo">  
    <li>
    <iframe 
	width="100%" 
	height="550" 
	class="share_self"  
	frameborder="0" 
	scrolling="no" 
	src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=0&ptype=1&speed=0&skin=1&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=1283533382&verifier=9bc28858&dpc=1">
    </iframe>
    </li>  
  </ul>  
</section>  

<section>
  <h1>WeChat</h1>
  <ul id="wechat">
  <div class=“pic” padding=“0”>
  <img src="/images/qrcode.jpg"><br />
  </div>
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2019 - Hexiaoqiao -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'Hexiaoqiao';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://hexiaoqiao.github.io/blog/2016/07/07/restart-cluster-ways-when-upgrade/';
        var disqus_url = 'http://hexiaoqiao.github.io/blog/2016/07/07/restart-cluster-ways-when-upgrade/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
