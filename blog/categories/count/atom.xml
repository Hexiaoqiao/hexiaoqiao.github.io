<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Count | Hexiaoqiao]]></title>
  <link href="http://hexiaoqiao.github.io/blog/categories/count/atom.xml" rel="self"/>
  <link href="http://hexiaoqiao.github.io/"/>
  <updated>2019-04-26T18:58:49+08:00</updated>
  <id>http://hexiaoqiao.github.io/</id>
  <author>
    <name><![CDATA[Hexiaoqiao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apache Kylin精确去重指标优化]]></title>
    <link href="http://hexiaoqiao.github.io/blog/2017/01/18/exact-count-optimization-of-apache-kylin/"/>
    <updated>2017-01-18T10:45:00+08:00</updated>
    <id>http://hexiaoqiao.github.io/blog/2017/01/18/exact-count-optimization-of-apache-kylin</id>
    <content type="html"><![CDATA[<p>前篇《<a href="http://hexiaoqiao.github.io/blog/2016/11/27/exact-count-and-global-dictionary-of-apache-kylin/">Apache Kylin精确计数与全局字典揭秘</a>》从精确计数使用场景到当前方案存在的问题及Apache Kylin在解决精确计数问题的思路进行了详细介绍。在此基础上，来自团队同学也是Apache Kylin社区Committer的<a href="https://github.com/kangkaisen">@Kangkaisen</a>更进一步，将Apache Kylin超高基数的精确去重指标查询提速数十倍。</p>

<h2>一、问题背景</h2>

<p>某业务方的Cube有12个维度，35个指标，其中13个是精确去重指标，并且有一半以上的精确去重指标单天基数在千万级别，Cube单天数据量1.5亿行左右。</p>

<p>但是一个结果仅有21行的精确去重查询竟然需要12秒多：<br/>
<code>
SELECT A, B, count(distinct uuid),
FROM table
WHERE dt = 17150
GROUP BY A, B
</code><br/>
跟踪整个查询执行过程发现，HBase端耗时~6s，Kylin的Query Server端耗时~5s。</p>

<p>精确去重指标已经在美团点评生产环境大规模使用，精确去重的查询的确比普通Sum指标慢，但是差异并不明显。但是这个查询的性能表现已超出预期，决定分析一下，到底慢在哪。</p>

<h2>二、优化过程</h2>

<h3>2.1 将精确去重指标拆分HBase列族</h3>

<p>首先确认了这个Cube的维度设计是合理的，这个查询也精准匹配了cuboid，并且在HBase端也只扫描了21行数据。</p>

<p>那么问题是，为什么在HBase端只扫描21行数据需要~6s？一个显而易见的原因是Kylin的精确去重指标是用bitmap存储的明细数据，而这个Cube有13个精确去重指标，并且基数都很大。</p>

<p>从两方面验证了这个猜想：<br/>
（1）同样SQL的查询Sum指标只需要120毫秒，并且HBase端Scan仅需2毫秒；<br/>
（2）用HBase HFile命令行工具查看并计算出HFile单个KeyValue的大小，发现普通的指标列族的每个KeyValue大小是29B，精确去重指标列族的每个KeyValue大小是37M；</p>

<p>所以第一个优化就是将精确去重指标拆分到多个HBase列族，优化后效果十分明显。查询时间从12s减少到5.7s，HBase端耗时从6s减少到1.3s，不过Query Server耗时依旧有~4.5s。</p>

<h3>2.2 移除不必要的toString避免bitmap deserialize</h3>

<p>Kylin的Query Server耗时依旧有4.5s，猜测还是和bitmap比较大有关，但是为什么bitmap大会导致如此耗时呢？</p>

<p>为了分析Query Server端查询处理的时间到底花在了哪，利用[Java Mission Control][1]进行了性能分析。</p>

<p>JMC分析很简单，在Kylin的启动进程中增加以下参数：
<code>
-XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints -XX:StartFlightRecording=delay=20s,duration=300s,name=kylin,filename=myrecording.jfr,settings=profile
</code>
获得myrecording.jfr文件后，在本机执行jmc，然后打开myrecording.jfr文件就可以进行性能分析。</p>

<p>热点代码的分析如图：</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/expensiveOftoString.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图1 JMC分析结果图示</label>
</div>


<p></p>

<p>从图中我们可以发现，耗时最多的代码是一个毫无意义的toString。
<code>
Preconditions.checkState(comparator.compare(last, fetched) &lt;= 0, "Not sorted! last: " + last + " fetched: " + fetched);
</code>
其中last和fetched就是一个bitamp。
去掉这个toString之后，Query Server的耗时减少超过1s。</p>

<h3>2.3 获取bitmap的字节长度时避免deserialize</h3>

<p>在去掉无意义的toString之后，热点代码已经变成了对bitmap的deserialize。</p>

<p>不过bitmap的deserialize共有两处，一处是bitmap本身的deserialize，一处是在获取bitmap的字节长度。</p>

<p>于是很自然的想法就是在获取bitmap的字节长度时避免deserialize bitmap，当时有两种思路：
（1）在serialize bitmap时就写入bitmap的字节长度；<br/>
（2）在MutableRoaringBitmap序列化的头信息中获取bitmap的字节长度。(Kylin的精确去重使用的bitmap是[RoaringBitmap][2])；</p>

<p>思路1中一个显然问题是如何保证向前兼容，这里向前兼容的方法就是根据MutableRoaringBitmap deserialize时的cookie头信息来确认版本，并在新的serialize方式中写入了版本号，便于之后序列化方式的更新和向前兼容。</p>

<p>经过这个优化后，Kylin Query Server端耗时再次减少超过1s。</p>

<h3>2.4 无需上卷聚合的精确去重查询优化</h3>

<p>从精确去重指标在美团点评大规模使用以来，我们发现部分用户的应用场景并没有跨Segment上卷聚合的需求，即只需要查询单天的去重值，或是每次全量构建的Cube，也无需跨Segment上卷聚合。</p>

<p>所以我们希望对无需上卷聚合的精确去重查询进行优化，当时考虑了两种可行方案：</p>

<p><strong>方案1：精确去重指标新增一种返回类型</strong></p>

<p>一个极端的做法是对无需跨segment上卷聚合的精确去重查询，我们只存储最终的去重值。</p>

<p>优点：<br/>
（1）存储成本会极大降低；<br/>
（2）查询速度会明显提高；</p>

<p>缺点：<br/>
（1）无法支持上卷聚合，与Kylin指标的设计原则不符合；<br/>
（2）无法支持segment的merge，因为要进行merge必须要存储明细的bitmap；<br/>
（3）新增一种返回类型，对不清楚的用户可能会有误导；<br/>
（4）查询需要上卷聚合时直接报错，用户体验不好，尽管使用这种返回类型的前提是无需上聚合卷；</p>

<p>实现难点：
如果能够接受以上缺点，实现成本并不高，目前没有想到明显的难点。</p>

<p><strong>方案2：serialize bitmap的同时写入distinct count值</strong></p>

<p>优点：<br/>
（1）对用户无影响；<br/>
（2）符合现在Kylin指标和查询的设计；</p>

<p>缺点：<br/>
（1）存储依然需要存储明细的bitmap；<br/>
（2）查询速度提升有限，因为即使不进行任何bitmap serialize，bitmap本身太大也会导致HBase scan，网络传输等过程变慢；</p>

<p>实现难点：
如何根据是否需要上卷聚合来确定是否需要serialize bitmap？开始的思路是从查询过程入手，确认在整个查询过程中，哪些地方需要进行上卷聚合。</p>

<p>为此，仔细阅读了Kylin Query Server端的查询代码，HBase Coprocessor端的查询代码，看了Calcite的example例子。发现在HBase端和Kylin Query Server端，Cube build时都有可能需要指标的聚合。</p>

<p>此时又意识到另外一个问题：即使清晰的知道了何时需要聚合，我又该如何把是否聚合的标记传递到精确去重的反序列方法中呢？</p>

<p>现在精确去重的deserialize方法参数只有一个ByteBuffer，如果加参数，就要改变整个kylin指标deserialize的接口，这将会影响所有指标类型，并会造成大范围的改动。所以把这个思路放弃了。</p>

<p>后来想到既然目标是优化无需上卷的精确去重指标，那为什么还要费劲去deserialize出整个bitmap呢，只要个distinct count值就可以。</p>

<p>所以目标就集中在BitmapCounter本身的deserialize上，并联想到早前在Kylin前端加载速度提升十倍的核心思想：延迟加载，就改变了BitmapCounter的deserialize方法，默认只读出distinct count值，不进行bitmap的deserialize，并将那个buffer保留，等到的确需要上卷聚合的时候再根据buffer deserialize 出bitmap。</p>

<p>当然，这个思路可行有一个前提，就是buffer内存拷贝的开销是远小于bitmap deserialize的开销，庆幸的是事实的确如此。</p>

<p>最终经过这个优化，对于无需上卷聚合的精确去重查询，查询速度也有了较大提升。</p>

<p>显然，这个优化加速查询的同时加大了需要上卷聚合的精确去重查询的内存开销。解决的办法：<br/>
（1）对于超大数据集并且需要上卷的精确去重查询，用户在分析查询时返回的结果行数应该不会太多；<br/>
（2）我们需要做好Query Server端的内存控制；</p>

<h2>三、总结</h2>

<p>通过总共4个优化，在向前兼容的前提下，后端仅通过100多行的代码改动，对Kylin超高基数的精确去重指标查询有了明显提升，测试中最明显的查询超过50倍的性能提升。</p>

<h2>四、反思</h2>

<p>（1）善于利用各类命令和工具，快速分析和定位问题；<br/>
（2）重写toString，hashCode，equals等基础方法一定要轻量化，避免复杂操作；<br/>
（3）设计序列化，通信协议，存储格式时，一定要有版本信息，便于之后的更新和兼容；</p>

<h2>五、参考</h2>

<p>[1] <a href="http://blog.takipi.com/oracle-java-mission-control-the-ultimate-guide/">http://blog.takipi.com/oracle-java-mission-control-the-ultimate-guide/</a> <br/>
[2] <a href="https://github.com/RoaringBitmap/RoaringBitmap">https://github.com/RoaringBitmap/RoaringBitmap</a><br/>
[3] <a href="https://issues.apache.org/jira/browse/KYLIN-2308">https://issues.apache.org/jira/browse/KYLIN-2308</a><br/>
[4] <a href="https://issues.apache.org/jira/browse/KYLIN-2337">https://issues.apache.org/jira/browse/KYLIN-2337</a><br/>
[5] <a href="https://issues.apache.org/jira/browse/KYLIN-2349">https://issues.apache.org/jira/browse/KYLIN-2349</a><br/>
[6] <a href="https://issues.apache.org/jira/browse/KYLIN-2353">https://issues.apache.org/jira/browse/KYLIN-2353</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Kylin精确计数与全局字典揭秘]]></title>
    <link href="http://hexiaoqiao.github.io/blog/2016/11/27/exact-count-and-global-dictionary-of-apache-kylin/"/>
    <updated>2016-11-27T10:45:00+08:00</updated>
    <id>http://hexiaoqiao.github.io/blog/2016/11/27/exact-count-and-global-dictionary-of-apache-kylin</id>
    <content type="html"><![CDATA[<p>Apache Kylin是基于Hadoop之上的SQL多维分析引擎，具有支持海量数据、秒级响应、高并发等特点。Kylin中使用的精确去重计数和全局字典技术在相关领域里非常具有借鉴意义。本文作者来自<a href="http://kylin.apache.org/">Apache Kylin</a>社区PMC成员<a href="https://github.com/sunyerui">@Sunyerui</a>，谢绝转载。</p>

<h2>一、问题背景</h2>

<p>在OLAP数据分析中，去重计数（count distinct）是非常常见的需求，且通常希望统计的结果是精确值，也就是说需要精确去重计数。</p>

<p>但在超大的数据集规模上，实现精确去重计数是一件非常困难的事，一句话总结：超大规模数据集下精确去重与快速响应之间的矛盾：</p>

<p>（1）超大规模数据集：意味着需要处理的数据量很多；<br/>
（2）精确去重：意味着要保留所有数据细节，这样才能使结果可上卷；<br/>
（3）快速响应：需要通过预计算或者内存计算等手段，加快速度；</p>

<p>上述条件相互约束，很难处理。因此目前主流OLAP系统都是采用近似去重计数的方式。</p>

<h2>二、现状分析</h2>

<p>在Apache Kylin中，目前提供的也是近似去重计数，是基于HLL（<a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>）实现的。</p>

<p>简单来说，每个需要被计数的值都会经过特定Hash函数的计算，将得到的哈希值放入到byte数组中，最后根据特定算法对byte数据的内容进行统计，就可以得到近似的去重结果。</p>

<p>这种方式的好处是，不论数据有多少，byte数组的大小是有理论上限的，通常不会超过128KB，存储压力非常小。也就是说能够满足超大数据集和快速响应的要求。</p>

<p>但最大的问题就是结果是非精确的，这是因为保存的都是经过hash计算的值，而一旦使用hash就一定会有冲突，这就是导致结果不精确的直接原因。此外在实践中发现，hash函数的计算是计算密集型的任务，需要大量的CPU资源。</p>

<p>目前Apache Kylin提供的近似去重计数最高精度误差为1.44%，这在很多场景下是不能满足需求的，这促使我们考虑是否能在现有框架下实现精确去重计数的功能。</p>

<h2>三、具体实现</h2>

<h3>3.1 Int型数据精确去重计数</h3>

<p>回顾文初提到三个问题，为了保证去重结果是精确，统计的结果必须要保留细节，而信息保存的最小单位是bit，也就是说在最理想情况下，每个用于统计的值都在结果中用一个bit来保存。这样的分析使我们很容易想到bitmap这种数据结构，它可以以bit为单位保存信息，且可以根据数据分布进行有效压缩，节省最后的存储量。但bitmap通常只能实现对int类型（包括byte和short类型）的处理，所以我们可以先尝试只针对Int型数据实现精确去重计数。</p>

<p>目前业界有很多成熟的bitmap库可用，比如RoaringBitmap，ConciseSet等，其中RoaringBitmap的读写速度最快，存储效率也很高，目前已经在Spark，Drill等开源项目中使用，因此我们也选择了RoaringBitmap。经过试验，证明了我们的想法是可行的，通过实现一种新的基于bitmap的去重计算指标，确实能够实现对Int型数据的精确去重计数。在百万量级的数据量下，存储的结果可以控制在几兆，同时查询能够在秒级内完成，且可以支持任意粒度的上卷聚合计算。</p>

<h3>3.2 Trie树与AppendTrie树</h3>

<p>上述的结果能够满足Int类型数据的需求，但更多的数据是其它类型的，比如String类型的uuid。那么是否能够采用类似方式支持其它类型的精确去重计数呢？答案是肯定的。只要能将所有数据都统一映射到int类型的数据，那么就可以同样采用bitmap的方式解决问题。这样的映射关系有很多种实现方式，比如基于格式的编码，hash编码等，其中空间和性能效率都比较高，且通用性更强的是基于Trie树的字典编码方式。目前Apache Kylin之前已经实现了TrieDictionary，用于维度值的编码。</p>

<p>Trie树又名前缀树，是是一种有序树，一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，根节点对应空字符串，而每个字符串对应的编码值由对应节点在树中的位置来决定。图1是一棵典型的Trie树示意图，注意并不是每个节点都有对应的字符串值。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/trie.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图1 Trie树结构图示</label>
</div>


<p></p>

<p>在构造Trie树时，将每个值依次加入到树中，可以分为三种情况，如图2所示。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/trieaddnode.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图2 构建Trie树过程</label>
</div>


<p></p>

<p>上述的是Trie树在构建过程中的内存模型，当所有数据加入之后，需要将整棵Trie树的数据序列化并持久化。具体的格式如图3所示。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/trieserialize.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图3 Trie树序列化图示</label>
</div>


<p></p>

<p>从图3中可以按照，整棵树按照广度优先的顺序依次序列化，其中childOffset和nValuesBeneath的长度是可变的，这是为了根据整棵树的大小尽可能使用更短的数据，降低存储量。此外需要注意到，childOffset的最高两位是标志位，分别标识当前节点是否是最后一个child，以及当前节点是否对应了原始数据的一个值。</p>

<p>当需要从字典中检索某个值的映射id时，直接从序列化后的数据读取即可，不需要反序列化整棵Trie树，这样能在保证检索速度的同时保持较低的内存用量。整个过程如图4所示。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/triesearch.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图4 Trie树检索过程</label>
</div>


<p></p>

<p>通过上述对Trie树的分析可以看到，Trie树的效率很高，但有一个问题，Trie树的内容是不可变的。也就是说，当一颗Trie树构建完成后，不能再追加新的数据进去，也不能删除现有数据，这是因为每个原始数据对应的映射id是由对应节点在树中的位置决定的，一旦树的结构发生变化，那么会导致部分原始数据的映射id发生变化，从而导致错误的结果。为此，我们需要对Trie树进行改造，使之能够持续追加数据，也就是AppendTrie树。</p>

<p>图5是AppendTrie的序列化格式，可以看到和传统Trie树相比，主要区别在于不保留每个节点的子value数，转而将节点对应的映射id保存到序列化数据中，这样虽然增大了存储空间，但使得整棵树是可以追加的。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/appendtrie.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图5 AppendTrie序列化图示</label>
</div>


<p></p>

<p>经过改造，AppendTrie树已经满足了我们的需求，能够支持我们实现对所有数据的统一编码，进而支持精确去重计数。但从实际情况来看，当统计的数据量超过千万时，整颗树的内存占用和序列化数据都会变得很大，因此需要考虑分片，将整颗树拆成多颗子树，通过控制每颗子树的大小，来实现整棵树的容量扩展。为了实现这个目标，当一棵树的大小超过设定阈值后，需要通过分裂算法将一棵树分裂成两棵子树，图6展示了整个过程。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/appendtriesplit.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图6 AppendTrie分裂过程图示</label>
</div>


<p></p>

<p>在一棵AppendTrie树是由多棵子树构成的基础上，我们很容易想到通过LRU之类的算法来控制所有子树的加载和淘汰行为。为此我们基于guava的LoadingCache实现了特定的数据结构CachedTreeMap。这种map继承于TreeMap，同时value通过LoadingCache管理，可以根据策略加载或换出，从而在保证功能的前提下降低了整体的内存占用。</p>

<p>此外，为了支持字典数据的读写并发，数据持久化采用mvcc的理念，每次构建持久化的结果作为一个版本，版本一旦生成就不可再更改，后续更新必须复制版本数据后进行，并持久化为更新的版本。每次读取时则选择当前最新的版本读取，这样就避免了读写冲突。同时设置了基于版本个数和存活时间的版本淘汰机制，保证不会占用过多存储。</p>

<p>经过上述的改进和优化后，AppendTrie树完全达到了我们的要求，可以对所有类型的数据统一做映射编码，支持的数据量可以到几十亿甚至更多，且保持内存占用量的可控，这就是我们的可追加通用字典AppendTrieDictionary。</p>

<h3>3.3 全局字典与全类型精确去重计数</h3>

<p>基于AppendTrieDictionary，我们可以将任意类型的数据放到一个字典里进行统一映射。在Apache Kylin的现有实现中，Cube的每个Segment都会创建独立的字典，这种方式会导致相同数据在不同Segment字典中被映射成不同的值，这会导致最终的去重结果出错。为此，我们需要在所有Segment中使用同一个字典实例，也就是全局字典GlobaDictionary。</p>

<p>具体来说，根据字典的资源路径（元数据名+库名+表名+列名）可以从元数据中获取同一个字典实例，后续的数据追加也是基于这个唯一的字典实例创建的builder进行的。</p>

<p>另外，经常出现一个cube中需要对多列计算精确去重，这些列的数据基于同一数据源，且所有列是其中某列数据的子集，比如uuid类型的数据，此时可以针对包含全集数据的列建一份全局字典，其它列复用这一个字典即可。</p>

<p>图7展示了整个字典构建到应用的全过程。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/kylin/appendtriesplit.png" alt="" align="center"><br />
<label class=“pic_title” align="center">图7 字典树构建及检索流程</label>
</div>


<p></p>

<h2>四、结论与展望</h2>

<p>通过对Bitmap和Trie树的合理运用和针对性优化，以及cache置换策略的应用，我们在Apache Kylin中成功实现了亿级规模的精确去重计数功能，支持任意粒度的上卷聚合，且保证内存可控。这里并没有全新的技术，只是对现有相关技术的组合运用，但解决了业界顶尖的技术难题。这也启发我们除了需要关注新技术，更应该多思考如何挖掘现有技术的潜力。</p>

<p>当然，目前的全局字典还存在一些问题和可改进的点，也欢迎更多讨论和贡献，其中包括：<br/>
（1）目前在内存有限，且字典较大时，容易出现字典分片被频繁换入换成的抖动，导致整体效率不高；<br/>
（2）全局字典只支持进程内的并发构建，但还不支持跨机器的并发构建；<br/>
（3）实际场景中，很多列的数据有高度相似性或属于同一来源，有可能共享同一个全局字典；</p>
]]></content>
  </entry>
  
</feed>
