<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hdfs | Hexiaoqiao]]></title>
  <link href="http://hexiaoqiao.github.io/blog/categories/hdfs/atom.xml" rel="self"/>
  <link href="http://hexiaoqiao.github.io/"/>
  <updated>2019-04-27T19:04:53+08:00</updated>
  <id>http://hexiaoqiao.github.io/</id>
  <author>
    <name><![CDATA[Hexiaoqiao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDFS锁机制优化方向讨论]]></title>
    <link href="http://hexiaoqiao.github.io/blog/2019/04/26/discussion-on-the-optimization-of-hdfs-global-lock-mechanism/"/>
    <updated>2019-04-26T10:45:00+08:00</updated>
    <id>http://hexiaoqiao.github.io/blog/2019/04/26/discussion-on-the-optimization-of-hdfs-global-lock-mechanism</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>众所周知，NameNode全局锁（FSNamesystemLock）问题一直是制约HDFS性能尤其是NameNode处理能力的主要原因。为此，社区和业界经过多次尝试，试图解决NameNode全局锁问题，但是从结果来看，都不理想。</p>

<p>本文将首先梳理NameNode当前的锁机制以及解决全局锁问题所面临的困难，结合经典分布式文件系统在这个问题上的一般解法，尝试给出可能的解决思路。</p>

<h2>二、全局锁机制</h2>

<p>NameNode是整个HDFS的核心组件<sup>[1]</sup>，集中管理HDFS集群的所有元数据，主要包括文件系统的目录树、数据块集合和分布以及整个集群的拓扑结构。</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/hausingqjm.png" align="center"><br />
<label class="pic_title" align="center">图1 HDFS HA using QJM架构图</label>
</div>


<p></p>

<p>同GFS一样HDFS采用了”一次写多次读“的读写模型来满足离线数据处理场景的存储需求，在此基础上，进一步放松一致性模型简化文件系统。在具体实现上，相比GFS1.0，HDFS做了更大胆取舍，锁机制上使用全局锁来统一来控制并发读写。这样处理的优势非常明显，全局锁进一步简化锁模型，不需要额外考虑锁依赖关系，同时降低复杂度，减少工程量。但是问题比优势更加突出，核心问题就是全局唯一锁制约性能提升。</p>

<p>为了更好地理解使用全局锁存在的问题，首先梳理全局锁管理的主要数据结构，大致分成三类：<br/>
（1）目录树：文件系统的全局目录视图。获取目录树上任一节点的信息必须先拿到全局读锁；目录树上任一节点新增、删除、修改都必须先拿到全局写锁。<br/>
（2）数据块集合：文件系统的全量数据信息。获取其中任一数据块信息必须先拿到全局读锁；新增、删除，修改都必须先拿到全局写锁。<br/>
（3）集群信息：HDFS集群节点信息的集合。获取节点信息等必须先拿到全局读锁；注册，下线或者变更节点信息请求处理时必须先拿到全局写锁。当然为了减少对全局影响，后续版本里少数如生命线等RPC请求不再获取全局锁，部分不适合使用全局锁的处理逻辑，将并发控制下放到具体的节点信息，尝试提升处理能力。</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/globallock.png" align="center"><br />
<label class="pic_title" align="center">图2 NameNode全局锁作用范围</label>
</div>


<p></p>

<p>具体实现上，NameNode使用了JDK提供的可重入读写锁（ReentrantReadWriteLock），我们知道ReentrantReadWriteLock对并行请求有严格限制，简单来说：读锁并行写锁排它。</p>

<p>针对不同RPC请求的处理逻辑，按照需要获取锁粒度，我们可以把所有请求抽象为读（Read Handler，获取全局读锁）和写（Write Handler，获取全局写锁）两类。<br/>
Read Handler：客户端请求（getListing/getBlockLocations/getFileInfo）、服务管理接口（monitorHealth/getServiceStatus）和主从节点之间请求（getTransactionID）等；<br/>
Write Handler：客户端请求（create/mkdir/rename/append/truncate/complete/recoverLease）、服务管理接口（transitionToActive/transitionToStandby/setSafeMode）和主从节点之间请求（rollEditLog）等；<br/>
这里只列了一些常用请求类型，其他如Cache/Snapshot/ACL/XAttr/Quota/Lease及NameNode内部线程调用等需要获取锁的逻辑没有再详细列出和归类。</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/concurrent.svg" align="center"><br />
<label class="pic_title" align="center">图3 NameNode全局锁对并行请求处理</label>
</div>


<p></p>

<p>NameNode的锁控制如图3所示。核心处理逻辑路径上有两把锁：FSNamesystemLock和FSEditLogLock。其中FSNamesystemLock即为通常所说的全局锁，采用ReentrantReadWriteLock机制实现；另外为了实现高可靠/高可用的目的，NameNode需要将对部分元数据的修改实时同步到EditLog，为了提升性能，EditLog读写不在FSNamesystemLock锁内执行，独立维护锁控制并行读写，暂称为FSEditLogLock，采用Synchronized排它机制实现。<br/>
（1）获取全局锁（FSNamesystemLock）入口：外部RPC请求从IPC层进入NameNode和内部线程请求；<br/>
（2）获取局部锁（FSEditLogLock）入口：主要来源外部RPC请求对元数据的写操作；</p>

<p>以RPC请求#mkdir为例：<br/>
（1）RPC请求从IPC层进入NameNode；<br/>
（2）获取全局写锁（FSNamesystemLock#writeLock#lock），如果持有读锁或者写锁的请求正在被处理，排队等待；<br/>
（3）更新内存目录树结构；<br/>
（4）释放全局写锁（FSNamesystemLock#writeLock#unlock）；<br/>
（5）获取EditLog排它锁；<br/>
（6）写EditLog；<br/>
（7）释放EditLog排它锁；<br/>
（8）通过IPC层将结果返回客户端；</p>

<p>可以看到，单个RPC请求处理流程经过了两次获取锁阶段。虽然二者相互独立，但其中任意一处如果不能及时获取到锁，RPC将处于排队等待状态，直到成功获得锁。等锁时间直接影响请求响应性能，极端场景下如果长时间不能获得锁，将造成IPC队列堆积，TCP连接队列被打满，客户端出现请求超时或者失败重试，新建连接超时失败等各种异常问题。<br/>
另外从全局来看，写锁因为排它对性能影响更加明显。如图3所示，如果当前有写请求正在被处理，其他所有请求都必须排队等待，直到写请求被处理完成释放锁后再竞争全局锁。</p>

<p>通常情况下，FSNamesystemLock锁范围要远大于FSEditLogLock锁范围。考虑负载较高的大规模集群，按照9:1读写比预估，只有10%请求需要同时获取FSNamesystemLock和FSEditLogLock，但是100%请求需要获取全局锁FSNamesystemLock。再加上新型硬件（SSD/3DPoint/PM）对IO性能的支持，EditLog写入性能远高于实际需求。所以从整体上看，当集群规模增加和负载增高后，全局锁FSNamesystemLock将逐渐成为NameNode性能瓶颈。如果能彻底解决NameNode全局锁问题，HDFS性能将得到极大提升。</p>

<h2>三、拆锁复杂度</h2>

<p>如前述，NameNode全局锁的拆分能带来非常可观的收益，Hadoop社区和业界也尝试过多次，但是从结果来看，效果都不理想。就我个人理解，其中问题复杂度客观存在，当然也有一些主观因素。总结下来有几个方面：</p>

<p><strong>1、问题复杂度</strong><br/>
Hadoop发展到今天已经超过十年，其中HDFS经过多次迭代演进，架构已经非常复杂。图4所示为HDFS项目包含和依赖的不完全组件列表，即使从事HDFS开发和运维的专业人员，想要完整了解和掌握HDFS的所有组件绝非易事。</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/modules.svg" align="center"><br />
<label class="pic_title" align="center">图4 HDFS不完全组件图</label>
</div>


<p></p>

<p>仅针对NameNode组件，架构上模块划分不够清晰，内部核心数据结构和工作线程之间耦合非常严重。比如：<br/>
（1）INodeFile通过对象引用关联Block，这种引用关系存在天然的耦合，很难通过不同锁进行并发访问控制；<br/>
（2）数据块写入完成后除了直接更新Block状态外，还需要再次回去更新文件属性，比如存储空间占用；<br/>
（3）在可靠性上使用到的FSImage和FSEditLog两份持久化数据内Namespace和BlocksMap数据共存；<br/>
实现细节上，还存在大量相互依赖，不一而足。</p>

<p>除了问题本身的复杂度，工程复杂度也比较高，据不完全统计，trunk分支上仅HDFS项目代码量超过1000K LOC，其中非测试代码量超过760K LOC，包括了超过2000类文件，要想优雅实现锁粒度拆分工程量很大。</p>

<p><strong>2、实际需求</strong><br/>
以社区版本branch-2.7为例，经过性能优化，NameNode处理能力可以达到5000TPS（写请求）或200000QPS（读请求），这种处理能力能够满足大多数公司的实际需求。如果负载超过这个量级一般也能通过Federation架构做横向扩展解决（虽然Federation架构在使用上会遇到很多意想不到的问题）。真正有实际需求，并需要尝试降低NameNode全局锁粒度解决性能问题的场景并不多。<br/>
<em>NOTE：性能数据是具体场景读写比例压测结果，不具备通用性，请谨慎参考。</em></p>

<p><strong>3、社区动力不足</strong><br/>
社区在全局锁和扩展性问题上做过多次尝试。比较有代表性的几类工作如下：</p>

<blockquote><p><a href="https://issues.apache.org/jira/browse/HDFS-8966">HDFS-8966</a>：Separate the lock used in namespace and block management layer<br/>
<a href="https://issues.apache.org/jira/browse/HDFS-5453">HDFS-5453</a>：Support fine grain locking in FSNamesystem<br/>
<a href="https://issues.apache.org/jira/browse/HDFS-8286">HDFS-8286</a>：Scaling out the namespace using KV store<br/>
<a href="https://issues.apache.org/jira/browse/HDFS-7836">HDFS-7836</a>：BlockManager Scalability Improvements</p></blockquote>

<p>几类方案中都描述了非常好的愿景，但是这些工作多数只推进了其中一部分，有的甚至还处于方案讨论阶段。总之，从几次尝试工作的结果来看，社区在这个方向上的动力并不足，投入有限。</p>

<p><strong>4、历史问题</strong><br/>
HDFS最初设计时为了实现简单方便做了很多取舍，其中全局锁是对后续的发展影响较大的一个。之后架构迭代中，大量工程实现都在全局锁基础上构建，确实对开发工作有很多便捷，但是如果想尝试梳理清楚和优雅拆分难度较大。</p>

<h2>四、拆锁讨论</h2>

<p>事实上，在分布式文件系统中，为实现解决数据一致性，通常都会不可避免遇到锁问题。不同的是，对于适合不同场景的文件系统，做的妥协或采用的方法有很大差异。借鉴成熟文件系统的锁模型，可以为HDFS拆锁工作提供一些参考和借鉴。其中Alluxio是非常好的参考对象，本章在调研Alluxio锁模型基础上，分析降低NameNode全局锁粒度的可能发展方向。</p>

<h3>4.1 Alluxio内存锁模型</h3>

<p>Alluxio<sup>[2]</sup>是一个基于内存的分布式文件系统，得益于云计算场景下的良好表现，被广泛部署和应用。<br/>
同HDFS类似，Alluxio也使用了Master-Slave的架构，其中Master管理Alluxio集群所有的元数据，包括目录树结构、数据块集合和分布及集群节点信息。实现上，FileSystemMaster负责管理整个目录树，BlockMaster管理数据块集合和分布，集群节点信息由BlockMaster中单独的集合数据结构mWorkers独立管理。</p>

<p>整体框架上与HDFS非常相似，但是具体到实现上，差异比较明显。<br/>
（1）FileSystemMaster和BlockMaster完全独立，通过blockid关联；<br/>
（2）mWorkers与FileSystemMaster/BlockMaster之间不存在复杂的耦合关系；<br/>
为了实现数据一致性，FileSystemMaster/BlockMaster/mWorkers之间独立加锁，以达到最好的并行性能。具体来看：</p>

<p>1、FileSystemMaster中目录树上所有节点各自维护读写锁（ReentrantReadWriteLock），控制并发读写：<br/>
（1）一元操作符：按照路径从根目录开始顺序加锁，写锁只加到最后一级目录，其他目录均加读锁；<br/>
（2）二元操作符：对公共目录非最后一级加读锁，最后一级根据操作符加读/写锁，剩余目录按照最后一级公共目录顺序加锁。<br/>
（3）为避免死锁，对于二元或者多元操作符先按路径排序，根据排序结果顺序对路径分别加锁；</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/dirlock.svg" align="center"><br />
<label class="pic_title" align="center">图5 Alluxio FileSystemMaster锁机制图示</label>
</div>


<p></p>

<p>2、BlockMaster完全独立于FileSystemMaster，核心数据结构mBlocks使用ConcurrentHashMap控制并发读写，具体到单个Block操作使用synchronized控制；</p>

<p>3、mWorker本身使用线程安全的集合数据结构管理，涉及到注册心跳等操作时，为每一个worker独立加锁；</p>

<p>从整个锁逻辑上看，有几点非常值得借鉴的地方：<br/>
（1）所有模块之间耦合度极低，核心逻辑不存在排它锁影响性能；<br/>
（2）为了将锁影响控制到最低，使用了大量在具体对象（block/worker）上加锁逻辑，而不是全局；</p>

<p>当然，凡是都有利弊，降低锁冲突提升性能一定是需要付出代价的：<br/>
（1）内存开销，因为在FileSystemMaster中目录树所有节点上独立使用读写锁（ReentrantReadWriteLock），会存在大量的内存对象的开销，制约Alluxio集群规模；在64bit环境上统计数据结构ReentrantReadWriteLock的footprint：<br/>
<figure class='code'><figcaption><span>ReentrantReadWriteLock footprint </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>java.util.concurrent.locks.ReentrantReadWriteLock@29444d75d footprint:
</span><span class='line'>     COUNT       AVG       SUM   DESCRIPTION
</span><span class='line'>         1        24        24   java.util.concurrent.locks.ReentrantReadWriteLock
</span><span class='line'>         1        48        48   java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync
</span><span class='line'>         1        16        16   java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock
</span><span class='line'>         1        16        16   java.util.concurrent.locks.ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter
</span><span class='line'>         1        16        16   java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock
</span><span class='line'>         5                 120   (total)<span class="nt">&lt;br/&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>对于一个10亿节点的目录树，仅ReentrantReadWriteLock对象的内存开销就将到~120GB，显然是一个巨大的开销。<br/>
（2）Alluxio的Master节点为了实现高可用，本身采用集群方式部署，为了保证一致性，所有元数据必须同步。这里涉及到FileSystemMaster/BlockMaster的Journal独立持久化逻辑，Alluxio实现时，将这部分逻辑都放在了锁内，对写请求处理的性能影响较大。</p>

<h3>4.2 GFS锁模型</h3>

<p>重新回顾GFS1.0是如何管理目录树和目录锁。下面是从论文《The Google File System》<sup>[4]</sup>中摘抄的有关目录树和锁机制的描述段落。</p>

<blockquote><p>Many master operations can take a long time: for example, a snapshot operation has to revoke chunkserver leases on all chunks covered by the snapshot. We do not want to delay other master operations while they are running. Therefore, we allow multiple operations to be active and use locks over regions of the namespace to ensure proper serialization. Unlike many traditional file systems, GFS does not have a per-directory data structure that lists all the files in that directory. Nor does it support aliases for the same file or directory (i.e, hard or symbolic links in Unix terms). GFS logically represents its namespace as a lookup table mapping full pathnames to metadata. With prefix compression, this table can be efficiently represented in memory. Each node in the namespace tree (either an absolute file name or an absolute directory name) has an associated read-write lock. Each master operation acquires a set of locks before it runs. Typically, if it involves /d1/d2/&hellip;/dn/leaf, it will acquire read-locks on the directory names /d1, /d1/d2, &hellip;, /d1/d2/&hellip;/dn, and either a read lock or a write lock on the full pathname /d1/d2/&hellip;/dn/leaf. Note that leaf may be a file or directory depending on the operation. We now illustrate how this locking mechanism can prevent a file /home/user/foo from being created while /home/user is being snapshotted to /save/user. The snapshot operation acquires read locks on /home and /save, and write locks on /home/user and /save/user. The file creation acquires read locks on /home and /home/user, and a write lock on /home/user/foo. The two operations will be serialized properly because they try to obtain conflicting locks on /home/user. File creation does not require a write lock on the parent directory because there is no “directory”, or inode-like, data structure to be protected from modification. The read lock on the name is sufficient to protect the parent directory from deletion. One nice property of this locking scheme is that it allows concurrent mutations in the same directory. For example, multiple file creations can be executed concurrently in the same directory: each acquires a read lock on the directory name and a write lock on the file name. The read lock on the directory name suffices to prevent the directory from being deleted, renamed, or snapshotted. The write locks on file names serialize attempts to create a file with the same name twice. Since the namespace can have many nodes, read-write lock objects are allocated lazily and deleted once they are not in use. Also, locks are acquired in a consistent total order to prevent deadlock: they are first ordered by level in the namespace tree and lexicographically within the same level.</p></blockquote>

<p>从这段描述里我们可以看到GFS对锁的管理：<br/>
（1）目录树中节点各自独立管理锁来控制并发；<br/>
（2）对锁模型更加激进，比如创建文件在整条路径上只使用读锁；<br/>
（3）为了避免死锁，对多元操作符按照路径排序后顺序加锁；<br/>
整体来看，Alluxion目录树锁机制与GFS锁机制异曲同工，将并行处理能力最大化。</p>

<h3>4.3 HDFS拆锁讨论</h3>

<p>借鉴和参考前面两类文件系统锁机制实现并结合HDFS现状，我个人认为HDFS降低全局锁粒度的可能发展路线：<br/>
<strong>1、垂直拆分<sup>[3]</sup></strong><br/>
NameNode内存几个核心数据结构里，DataNodeManager管理的内容相对独立，比较容易独立拆分出去，事实上社区现在基本完成了这个工作，下面只考虑两个核心数据结构Namespace和BlocksMap：<br/>
（1）按照HDFS Federation架构的思路，在单NameNode进程内实施Federation；<br/>
（2）将Namespace按照Range进行垂直切分；<br/>
（3）Namespace变化成两级管理结构；<br/>
<figure class='code'><figcaption><span>Double-level-struction </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>RangeMap：Range-GSet
</span><span class='line'>GSet：key-INode/BlockInfo
</span></code></pre></td></tr></table></div></figure><br/>
（4）Range内独享锁，Range之间可并行访问；<br/>
（5）跨Range多元操作符按照Range排序后顺序加锁避免死锁；<br/>
（6）当单进程整体负载较高时，Range重新分配独立进程，实现动态切分目录树的效果；</p>

<p>目录树的垂直切分思路到最后可以跟HDFS Federation很好的结合起来（虽然HDFS Federation架构存在很多问题）实现类似Ceph中简化版Dynamic Subtree Partitioning目标。</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/layerlock.png" align="center"><br/>
</div>


<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/partitiontree.png" align="center"><br/>
<label class=“pic_title” align="center">图6 NameNode全局锁垂直切分</label>
</div>


<p></p>

<p><strong>2、水平拆分</strong><br/>
NameNode全局锁水平拆分的思路可以借鉴GFS1.0和Alluxio解决思路，按照两个阶段降低NameNode锁粒度：<br/>
第一阶段：对NameNode核心数据结构进行分层解耦，不同层独立持锁；<br/>
第二阶段：降低Namespace层锁粒度；</p>

<p>第一阶段分层解耦：<br/>
（1）Namespace层维护与目录树有关的所有数据结构（INodeMap，Lease等），核心是INodeMap，目录树文件节点上通过List<Long> BlockIds即数据块序号维护与数据块的关系，取代对象索引；<br/>
（2）BlocksManager层维护与数据块相关的所有数据结构（BlocksMap，ReplicationMonitor，NetworkTopology等），核心是BlocksMap：GSet&lt;BlockId, BlockInfo>；将副本数和存储策略等与数据块有关的属性统一下沉到BlockInfo内，降低Namespace与BlocksManager的耦合；（一部分工作社区已经完成）<br/>
（3）DataNodeManager层仅维护集群节点数据结构，不维护拓扑结构（非重点，当前的实现已经不在锁内）；<br/>
（4）每一层维护独立锁，开放接口以线程安全方式对外暴露。</p>

<p>拆分后同一进程内会出现多把独立锁，不可避免会存在锁内相互调用的问题，为了避免出现死锁，可以做简单约束：<br/>
（1）单次请求处理涉及数据结构<Namespace>, <BlocksMap>或者&lt;Namespace,BlocksMap>；<br/>
（2）尽可能减少或避免锁内跨层调用（如Alluxio）；<br/>
（3）特殊场景需要锁内跨层调用时，仅允许Namespace到BlocksMap单向调用；</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/exlayerlock.png" align="center"><br />
<label class="pic_title" align="center">图7 NameNode全局锁水平切分</label>
</div>


<p></p>

<p>第二阶段降低锁粒度：<br/>
（1）目录树全局锁下沉到目录树节点，不过如前述因为ReentrantReadWriteLock的footprint较大，直接使用容易造成内存瓶颈。可以选择以下优化和改进：<br/>
  * 按照满足读写锁能力的最小资源重新实现Lock，降低整颗目录树节点使用锁后的内存占用；<br/>
  * 维护独立的目录锁动态子树；因为NameNode进程内提供的请求处理线程数有限，目录锁子树规模非常小，几乎没有管理和遍历的成本；<br/>
（2）写锁仅持有写操作的最后一级目录，其他父目录均加读锁；<br/>
（3）多元操作符按照请求目录排序后顺序加锁避免死锁；</p>

<div class="pic" align="center" padding="0">
<img src="http://hexiaoqiao.github.io/images/hdfslock/exdirloc.svg" align="center"><br />
<label class="pic_title" align="center">图8 目录树全局锁下沉到节点 </label>
</div>


<p></p>

<p>全局操作类型，比如safemode/haadmin/metasave因为都是superuser类请求，频率非常低，不需要再维护独立锁。为了简化，对大部分superuser管理类型的请求可以同时获取两把写锁，对整体性能不会有影响。</p>

<h2>五、总结</h2>

<p>NameNode全局锁一直是影响HDFS性能的关键问题，尽管社区在这方面做过多次尝试，但是结果都不是很理想。其中的问题难度客观存在。<br/>
（1）HDFS架构快速迭代和演进，丰富的功能和更加复杂组件让HDFS内部模块之间存在千丝万缕的耦合关系，完全梳理清楚成本较高；<br/>
（2）HDFS项目代码量除单元测试外接近780K LOC，工程量很大；<br/>
（3）设计之初为实现简单做了很多取舍，比如全局锁及在此基础上的大量工程实现（“战术的勤奋掩盖战略的懒惰”的实例）；<br/>
虽有难度但也存在办法，本文在参考其他分布式文件系统锁模型的基础上，结合当前HDFS实际情况和业界正在尝试的方向，期望提供两种降低全局锁粒度的思路和可能演进方向，两种演进方向相互没有依赖，可以并行演进。</p>

<p>当然，提升性能或者扩展能力，拆分NameNode全局锁并不是唯一解。比如由LinkedIn和Hortorworks分别在推进的Observer NameNode和OZone都是非常好的思路。<br/>
Observer NameNode通过开放Standby读能力提升NameNode整体QPS。<br/>
Hadoop OZone通过引入对象存储思路，将文件系统的元数据进行分解下沉，期望能够实现良好的性能和扩展能力。</p>

<h2>六、参考</h2>

<p>[1] <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a><br/>
[2] <a href="https://www.alluxio.org/">https://www.alluxio.org/</a><br/>
[3] <a href="https://engineering.linkedin.com/blog/2019/02/the-present-and-future-of-apache-hadoop--a-community-meetup-at-l">https://engineering.linkedin.com/blog/2019/02/the-present-and-future-of-apache-hadoop--a-community-meetup-at-l</a><br/>
[4] <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf">https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS BlockToken机制解析]]></title>
    <link href="http://hexiaoqiao.github.io/blog/2018/07/13/a-brief-introduction-of-hdfs-blocktoken-mechanism/"/>
    <updated>2018-07-13T10:45:00+08:00</updated>
    <id>http://hexiaoqiao.github.io/blog/2018/07/13/a-brief-introduction-of-hdfs-blocktoken-mechanism</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>敏感信息和隐私数据的安全保障是互联网公司非常关心的问题，尤其进入大数据时代，稍有不慎就会出现重大安全事故，所以数据安全问题就变得越来越重要。</p>

<p>Hadoop作为数据平台的基础设施，需要优先关注和解决好安全问题。虽然安全特性对Hadoop非常重要，不过社区直到2011年末随Hadoop-1.0.0才第一次正式发布Hadoop Security，在这之前Hadoop社区版存在较大的安全隐患，需要用户自行解决。</p>

<p>当然数据安全本身是一个复杂的系统工程，想要描述清楚和完美解决几乎不可能。尽管如此，合理有效的安全保障是必要的。本文就Hadoop中数据块安全问题，从设计权衡和实现原理进行简单分析和梳理，简要阐述当前方案在实践中可能遇到的问题，同时提供可借鉴的解决思路。</p>

<h2>二、Hadoop安全概述</h2>

<p>Hadoop安全需要解决两个问题：<br/>
（1）认证：解决用户身份合法性验证问题；<br/>
（2）授权：解决认证用户的操作范围问题；<br/>
其中认证问题通过Kerberos能够很好地解决，并通过<a href="https://issues.apache.org/jira/browse/HADOOP-4487">HADOOP-4487</a>在Hadoop内部设计了一套Token机制完美实现了安全认证问题，同时在性能上得到保证，图1为Hadoop安全认证体系概要图示。关于Hadoop Security特性的细节参考<a href="https://issues.apache.org/jira/browse/HADOOP-4487">HADOOP-4487</a>，这里不再展开。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/blocktoken/security.png" align="center"><br />
<label class=“pic_title” align="center">图1 Hadoop安全认证体系</label>
</div>


<p></p>

<p>社区针对这个问题在2008.10与Hadoop Security特性同步开始设计BlockToken方案HADOOP-4359，经过半年左右时间在2009.05完成并发布，BlockToken特性可以非常好地保护数据块安全。可以说HADOOP-4487和HADOOP-4359构建起整个Hadoop安全体系，本文重点关注HADOOP-4359。</p>

<p>社区针对这个问题在2008.10与Hadoop Security特性同步开始设计BlockToken方案<a href="https://issues.apache.org/jira/browse/HADOOP-4359">HADOOP-4359</a>，经过半年左右时间在2009.05完成并发布，通过BlockToken数据块安全问题也得到了很好的解决。可以说<a href="https://issues.apache.org/jira/browse/HADOOP-4487">HADOOP-4487</a>和<a href="https://issues.apache.org/jira/browse/HADOOP-4359">HADOOP-4359</a>构建起了整个Hadoop安全体系。</p>

<h2>三、安全基础简介</h2>

<p>BlockToken方案使用HMAC（Hash Message Authentication Code）[1]技术实现对合法请求的访问认证检查。</p>

<p>HMAC是一种基于HASH函数和共享密钥的消息安全认证协议，它可以有效地防止数据在传输的过程中被截取和篡改，维护数据的安全性、完整性和可靠性。HMAC可以与任何迭代HASH函数结合使用，MD5和SHA-1就是这种HASH函数。实现原理是用公开函数和共享密钥对原始数据产生一个固定长度的值作为认证标识，用这个标识鉴别消息的完整性。使用密钥生成一个固定大小的消息摘要小数据块即HMAC，并加入到消息中一起传输。接收方利用与发送方共享的密钥对接收到的消息进行认证和合法性检查。这种算法不可逆，无法通过消息摘要反向推导出消息，因此又称为单向HASH函数。通过这种技术可以有效保证数据的安全性、完整性和可靠性。</p>

<p>HMAC算法流程：
（1）消息传递前，Alice和Bob约定共享密钥和HASH函数；
（2）Alice把要发送的消息使用共享密钥计算出HMAC值，然后将消息和HMAC发送给Bob；
（3）Bob接收到消息和HMAC值后，使用共享密钥独立计算消息本身的HMAC值，与接收到的HMAC值对比；
（4）如果二者的HMAC值相同，说明接收到的消息是完整的，且是Alice发送；</p>

<p>BlockToken方案默认使用了经典的HMAC-SHA1算法，对照前面的流程，Alice代表的是NameNode，Bob代表DataNode，客户端在整个过程中仅作为数据流转的节点。因为HMAC能够保证数据传输过程中不被截取和篡改，只要NameNode给客户端发放了BlockToken，即可认为该客户端申请对单个数据块的访问权限是可信赖的，DataNode只要对BlockToken检查通过就必须接受客户端表述的所有权限。</p>

<h2>四、HDFS BlockToken机制</h2>

<p>Token机制是整个Hadoop生态里安全协议的重要组成部分，在HDFS内部包括两个部分：<br/>
（1）客户端经过初始认证（Kerberos），从NameNode获取DelegationToken，作为后续访问HDFS的凭证；<br/>
（2）客户端读写数据前，请求NameNode获取对应数据块Block信息和BlockToken，根据结果向对应DataNode真正请求读写数据。请求到达DataNode端，根据客户端提供的BlockToken进行安全认证检查，通过后继续后续步骤，否则请求失败；</p>

<p>第二部分就是<a href="https://issues.apache.org/jira/browse/HADOOP-4359">HADOOP-4359</a>和本文主要关注的内容。</p>

<h3>4.1 HDFS读写流程</h3>

<p>开始详细梳理BlockToken原理之前，首先简单梳理下如图2所示的HDFS读写流程：</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/blocktoken/read.png" align="center">
<img src="http://hexiaoqiao.github.io/images/blocktoken/write.png" align="center"><br />
<label class=“pic_title” align="center">图2 HDFS读写流程示意图</label>
</div>


<p></p>

<p>（1）客户端读写操作（open/create）需首先获取数据块Block分布，根据文件路径请求NameNode获取LocatedBlock；<br/>
（2）如果是读操作，根据返回LocatedBlock集合，从中选择合适的DataNode进行读数据请求，若需要读取的数据分布在多个Block，按顺序逐个切换到对应DataNode读取；<br/>
（3）如果是写操作，首先将返回的LocatedBlock中所有DataNode建立数据管道（Pipeline），然后开始向数据管道里写数据，若写出的数据不能在一个Block内完成，再次向NameNode申请LocatedBlock，直到所有数据成功写出；<br/>
（4）读写操作完成，关闭数据流；</p>

<p>LocatedBlock是衔接整个读写流程的关键数据结构：</p>

<p><figure class='code'><figcaption><span>LocatedBlock.java </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">LocatedBlock</span> <span class="o">{</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kd">final</span> <span class="n">ExtendedBlock</span> <span class="n">b</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kt">long</span> <span class="n">offset</span><span class="o">;</span>  <span class="c1">// offset of the first byte of the block in the file</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kd">final</span> <span class="n">DatanodeInfoWithStorage</span><span class="o">[]</span> <span class="n">locs</span><span class="o">;</span>
</span><span class='line'>  <span class="o">/&lt;</span><span class="n">strong</span><span class="o">&gt;</span> <span class="n">Cached</span> <span class="n">storage</span> <span class="n">ID</span> <span class="k">for</span> <span class="n">each</span> <span class="n">replica</span> <span class="o">*/</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">String</span><span class="o">[]</span> <span class="n">storageIDs</span><span class="o">;</span>
</span><span class='line'>  <span class="o">/&lt;/</span><span class="n">strong</span><span class="o">&gt;</span> <span class="n">Cached</span> <span class="n">storage</span> <span class="n">type</span> <span class="k">for</span> <span class="n">each</span> <span class="n">replica</span><span class="o">,</span> <span class="k">if</span> <span class="n">reported</span><span class="o">.</span> <span class="o">*/</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">StorageType</span><span class="o">[]</span> <span class="n">storageTypes</span><span class="o">;</span>
</span><span class='line'>  <span class="c1">// corrupt flag is true if all of the replicas of a block are corrupt.</span>
</span><span class='line'>  <span class="c1">// else false. If block has few corrupt replicas, they are filtered and</span>
</span><span class='line'>  <span class="c1">// their locations are not part of this object</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kt">boolean</span> <span class="n">corrupt</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">Token</span><span class="o">&lt;</span><span class="n">BlockTokenIdentifier</span><span class="o">&gt;</span> <span class="n">blockToken</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Token</span><span class="o">&lt;</span><span class="n">BlockTokenIdentifier</span><span class="o">&gt;();</span>
</span><span class='line'>  <span class="o">&amp;</span><span class="n">hellip</span><span class="o">;&amp;</span><span class="n">hellip</span><span class="o">;</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<h3>4.2 BlockToken数据结构</h3>

<p>前一节提到的LocatedBlock除了标识数据块Block信息外，还包含了认证流程中的核心数据结构blockToken：  <br/>
<figure class='code'><figcaption><span>Token.java </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Token</span><span class="o">&lt;</span><span class="n">T</span> <span class="kd">extends</span> <span class="n">TokenIdentifier</span><span class="o">&gt;</span> <span class="kd">implements</span> <span class="n">Writable</span> <span class="o">{</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kt">byte</span><span class="o">[]</span> <span class="n">identifier</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kt">byte</span><span class="o">[]</span> <span class="n">password</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">Text</span> <span class="n">kind</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">Text</span> <span class="n">service</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">TokenRenewer</span> <span class="n">renewer</span><span class="o">;</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>blockToken的主要属性如下：<br/>
（1）kind标识的是Token的类型，这里为常量“HDFS_BLOCK_TOKEN”；<br/>
（2）service用来描述请求的服务，一般由服务端的”host:port”组成，对blockToken一般置空；<br/>
（3）TokenRenewer在客户端生命周期内周期Renew，避免因为Token过期造成请求失败，对BlockToken未见Renew的显性实现，所以BlockToken只在有效期内生效；<br/>
（4）identifier是BlockTokenIdentifier的序列化结果：<br/>
<figure class='code'><figcaption><span>BlockTokenIdentifier.java </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">BlockTokenIdentifier</span> <span class="kd">extends</span> <span class="n">TokenIdentifier</span> <span class="o">{</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kt">long</span> <span class="n">expiryDate</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kt">int</span> <span class="n">keyId</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">String</span> <span class="n">userId</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="n">String</span> <span class="n">blockPoolId</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kt">long</span> <span class="n">blockId</span><span class="o">;</span>
</span><span class='line'>  <span class="kd">private</span> <span class="kd">final</span> <span class="n">EnumSet</span><span class="o">&lt;</span><span class="n">AccessMode</span><span class="o">&gt;</span> <span class="n">modes</span><span class="o">;</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>包含了当前请求来源userId，数据块标识blockId，数据块所在的BlockPool（用于HDFS Federation架构），本次请求的权限标识modes（READ, WRITE, COPY, REPLACE），Token的过期时间及keyId；<br/>
（5）password即是使用共享密钥SecretKey应用HMAC算法对identifier计算得到的密码。<br/>
需要说明的是，keyId和SecretKey存在对应关系，通过keyId可以索引到SecretKey，后续详细介绍。</p>

<h3>4.3 BlockToken流程</h3>

<p>BlockToken体现在HDFS读写流程的以下几个步骤里：</p>

<p>1、客户端使用文件路径向NameNode发送读写请求，其中请求接口如下：</p>

<p><figure class='code'><figcaption><span>interface </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="n">LocatedBlocks</span> <span class="nf">getBlockLocations</span><span class="o">(</span><span class="n">String</span> <span class="n">clientName</span><span class="o">,</span> <span class="n">String</span> <span class="n">src</span><span class="o">,</span> <span class="kt">long</span> <span class="n">offset</span><span class="o">,</span> <span class="kt">long</span> <span class="n">length</span><span class="o">);</span>
</span><span class='line'><span class="kd">public</span> <span class="n">LocatedBlock</span> <span class="nf">addBlock</span><span class="o">(</span><span class="n">String</span> <span class="n">src</span><span class="o">,</span> <span class="n">String</span> <span class="n">clientName</span><span class="o">,</span> <span class="n">ExtendedBlock</span> <span class="n">previous</span><span class="o">,</span> <span class="n">DatanodeInfo</span><span class="o">[]</span> <span class="n">excludedNodes</span><span class="o">,</span> <span class="kt">long</span> <span class="n">fileId</span><span class="o">,</span> <span class="n">String</span><span class="o">[]</span> <span class="n">favoredNodes</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>2、NameNode经过权限检查后，搜索到文件对应的数据块信息，结合激活的keyId组织出完整的BlockTokenIdentifier，使用keyId对应密钥SecretKey加密BlockTokenIdentifier得到密码，BlockToken数据就绪，加上已经获取到的数据块信息即是LocatedBlock返回给客户端；</p>

<p>3、客户端从NameNode获取到LocatedBlock后，带着BlockToken请求对应DataNode执行数据读写操作；</p>

<p>4、DataNode端接收到读写请求，首先进行BlockToken检查，目的是检查客户端的真实性和权限。主要有两个步骤：<br/>
（1）将BlockToken里的identifier反序列化，检查客户端请求的数据块、访问权限及用户名是否与BlockToken的表达一致，如果检查通过进入下一步，否则直接失败；<br/>
（2）从identifier反序列化结果里取出keyId，在本地索引对应的共享密钥SecretKey，使用与NameNode端相同的HMAC算法计算password，之后与BlockToken中的password进行比较，如果相等开始真正的数据读写流程，否则请求失败。</p>

<p>上述流程中，NameNode和DataNode计算密码时使用的密钥SecretKey均是以BlockTokenIdentifier.keyid作为索引在本地内存中获取。要想对相同的BlockTokenIdentifier使用同样的加密算法计算得到相同的结果，密钥SecretKey必须完全一致。所以核心问题是，NameNode和DataNode如何保证密钥SecretKey同步，使符合预期的请求通过验证。</p>

<p>最简单的办法就是NameNode和DataNode初始化固定的密钥，到期后NameNode重新生成并同步给DataNode问题解决。</p>

<p>但是事实并没有这么简单，我们知道DataNode与NameNode之间信息交互最频繁的渠道是Heartbeat（默认3s一次），如果NameNode更新了SecretKey，但是DataNode心跳3s后才上报，在这3s时间内，两端存在密钥不一致的问题，也就是在这个时段内即使合法请求也会检查失败，所以“最简单的办法”显然还不能完全解决问题。</p>

<p>虽然“最简单的办法”存在问题，但是提供了一种简单高效解决问题的思路，既然只维护一份共享密钥SecretKey会出现“黑障区”问题，那么同一时刻始终保持两份在线，这样就可以完全避免3s的黑障时间段。</p>

<p>事实上，HDFS更进一步同时维护三份共享密钥，NameNode一旦发现有SecretKey过期，马上生成新SecretKey补充进来并向前滚动当前激活SecretKey，DataNode心跳过来后及时下发更新后的SecretKey集合，如图3所示。维护三份密钥的代价是NameNode需要同时检查三份数据有效期，但是通常情况过期时间较大（默认是10h）且数据量极小，所以完全不会给NameNode或者DataNode带来负担。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/blocktoken/blocktoken.png" align="center"><br />
<label class=“pic_title” align="center">图3 HDFS BlockToken流程图示</label>
</div>


<p></p>

<h3>4.4 BlockToken密钥HA</h3>

<p>前面提到了NameNode和DataNode同步密钥的流程，在HDFS HA架构里通常还存在Active NameNode和Standby NameNode同步数据的问题。</p>

<p>事实上，Active与Standby之间不对SecretKey通过EditLog或其他方式同步。这样带来的新问题是：如何保证操作主从切换后，当前正常读写请求的Token验证通过。如前面提到，NameNode定期更新SecretKey后及时将更新后的SecretKey集合同步给DataNode，DataNode更新以保证正常读写请求通过验证，这种方式对Active和Standby同样适用。所以单从DataNode来看，同一个BlockPool实际上同一时间本地缓存至少6份共享密钥，其中3份来自Active NameNode，另外3份来自Standby NameNode。这样的话，不管客户端请求携带的keyId来自Active NameNode或者Standby NameNode，只要是正常请求均能验证通过，与是否操作主从切换或者从Standby NameNode请求无关。</p>

<p>接下来的问题，DataNode维护了多份&lt;keyId,SecretKey>数据，如何避免来自Active和Standby之间的keyId冲突，以及HDFS Federation架构下，来自多个Namespace的keyId冲突。先来看HDFS Federation架构，与BlockPool类似，共享密钥相关信息也按照这个维度组织就不会相互干扰。来自同Namespace下Active和Standby的keyId确实存在冲突的可能，为了避免出现这种情况，实现时结合Active和Standby的nnId分配独立的keyId序号段即可解决。</p>

<p>除了以上问题，服务重启时还存在其他问题：<br/>
（1）NameNode重启：当NameNode重启会重置，由于NameNode重启后所有DataNode需要重新注册，注册完成后返回的CMD指令中包含了NameNode的集合，保证了DataNode与NameNode之间完成同步；<br/>
（2）DataNode重启：DataNode重启比较简单，向Active NameNode和Standby NameNode分别注册，成功后会收到Active和Standby的所有集合，更新内存状态即可。</p>

<p>为什么NameNode之间不像其他的WRITE操作，通过EditLog在Active与Standby之间保持同步？原因有两个：<br/>
1、SecretKey更新频率很低（10h）；<br/>
2、数据量非常小（可忽略）；<br/>
根据这两条不管是NameNode端还是DataNode端都完全可以承载，另外如果通过EditLog同步会增加复杂度，同时如果持久化SecretKey安全性上大打折扣，与Token设计的初衷相悖。</p>

<p>至此，BlockToken的整个流程简单梳理完成，可以看出BlockToken与Kerberos体系的架构和核心流程有很多相似的地方。</p>

<h2>五、BlockToken的问题及解决思路</h2>

<p>前面BlockToken流程分析可以看出，设计思路和实现方案都比较优雅，但是实践过程中还是可能会遇到一些问题：<br/>
（1）NameNode重启完成后DataNode没有成功更新SecretKey造成客户端读写失败；<br/>
（2）NameNode滚动SecretKey后DataNode没有及时同步造成后续读写失败；<br/>
<figure class='code'><figcaption><span>datanode.log </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>2017-05-17 23:41:58,952 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hostname:50010:DataXceiver error processing WRITE_BLOCK operation  src: /ip:port
</span><span class='line'>dst: /ip:port
</span><span class='line'>org.apache.hadoop.security.token.SecretManager$InvalidToken: Can<span class="ni">&amp;rsquo;</span>t re-compute password for block_token_identifier (expiryDate=<span class="nt">&lt;em&gt;</span>, keyId=<span class="nt">&lt;/em&gt;</span>, userId=<span class="nt">&lt;em&gt;</span>, blockPoolId=<span class="nt">&lt;/em&gt;</span>, blockId=<span class="nt">&lt;em&gt;</span>, access modes=[WRITE]), since the required block key (keyID=<span class="nt">&lt;/em&gt;</span>) doesn<span class="ni">&amp;rsquo;</span>t exist.
</span><span class='line'>       at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.retrievePassword(BlockTokenSecretManager.java:384)
</span><span class='line'>       at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:302)
</span><span class='line'>       at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)
</span><span class='line'>       at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1271)
</span><span class='line'>       at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:663)
</span><span class='line'>       at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
</span><span class='line'>       at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
</span><span class='line'>       at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
</span><span class='line'>       at java.lang.Thread.run(Thread.java:745)
</span></code></pre></td></tr></table></div></figure></p>

<p>这两个问题的主要原因是社区实现中DataNode同步SecretKey采用的是从NameNode Push的方案，但是对是否Push成功没有感知，比如：</p>

<p>（1）NameNode重启后，会重新生成新SecretKey集合，DataNode注册时NameNode将所有新生成的SecretKey集合Push给DataNode。我们知道NameNode重启阶段负载非常高，尤其是大规模集群，存在一种情况是NameNode端成功处理了DataNode的注册请求，并将SecretKey集合返回给DataNode，但是DataNode端已经超时没有接收到NameNode的返回结果，这个时候NameNode和DataNode两端出现不一致：NameNode认为DataNode已经成功更新了SecretKey，之后不再下发更新SecretKey命令，但是DataNode端没有接收到新SecretKey集合，依然维护一批无效SecretKey。此后当客户端读写请求过来后，BlockToken验证永远失败；</p>

<p>（2）NameNode滚动SecretKey后，通过Heartbeat的返回值将新SecretKey集合Push给DataNode，同前述场景类似，返回值超时或者DataNode没有接收到心跳的返回值，同样造成NameNode和DataNode两端密钥不一致，默认最长10h后该keyId被激活时，客户端的请求因为BlockToken验证失败同样会读写失败；</p>

<p>前面的两类场景可以看出，问题实际上发生在NameNode向DataNode同步SecretKey，由于采用了Push的方案，但是对结果是否正常并没有感知，两端的数据不一致造成。对应解决方案其实也比较清晰，将NameNode向DataNode同步SecretKey的实现从Push改为Pull，该方案已在社区讨论详见<a href="https://issues.apache.org/jira/browse/HDFS-13473">HDFS-13473</a>：<br/>
（1）DataNode注册时通过NameNode发下命令更新SecretKey的处理流程保持现状；<br/>
（2）在DataNode的心跳中增加当前SecretKey的版本号，NameNode端如果发现与本地SecretKey版本号不匹配通过心跳返回最新SecretKey集合；</p>

<p>将SecretKey同步方式从Push更新到Pull之后，因为心跳间隔默认3s，即使存在单次甚至连续数次心跳处理失败，也可以在接下来成功的请求里及时更新，而不再是必须等默认10h之后才能再次发起同步，而且依然存在更新不成功的可能。可以有效避免NameNode和DataNode两端因为SecretKey不一致造成客户端读写请求失败的问题。</p>

<h2>六、总结</h2>

<p>本文以Hadoop Security特性背景入手，对HDFS BlockToken方案设计的考虑，社区实现原理，存在的问题和解决思路进行了简单分析和梳理。<br/>
通过对BlockToken机制原理和实现细节解析，期望对Hadoop安全窥一斑见全局，对其中可能存在的问题及优化思路提供参考价值。</p>

<h2>七、参考</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/HMAC">https://en.wikipedia.org/wiki/HMAC</a><br/>
[2] <a href="https://issues.apache.org/jira/secure/attachment/12428537/security-design.pdf">https://issues.apache.org/jira/secure/attachment/12428537/security-design.pdf</a><br/>
[3] <a href="https://issues.apache.org/jira/secure/attachment/12409284/AccessTokenDesign1.pdf">https://issues.apache.org/jira/secure/attachment/12409284/AccessTokenDesign1.pdf</a><br/>
[4] <a href="https://issues.apache.org/jira/browse/HADOOP-4487">https://issues.apache.org/jira/browse/HADOOP-4487</a><br/>
[5] <a href="https://issues.apache.org/jira/browse/HADOOP-4359">https://issues.apache.org/jira/browse/HADOOP-4359</a><br/>
[6] <a href="https://issues.apache.org/jira/browse/HDFS-13473">https://issues.apache.org/jira/browse/HDFS-13473</a><br/>
[7] <a href="http://hadoop.apache.org/releases.html">http://hadoop.apache.org/releases.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS HA Using QJM原理解析]]></title>
    <link href="http://hexiaoqiao.github.io/blog/2018/03/30/the-analysis-of-basic-principle-of-hdfs-ha-using-qjm/"/>
    <updated>2018-03-30T10:45:00+08:00</updated>
    <id>http://hexiaoqiao.github.io/blog/2018/03/30/the-analysis-of-basic-principle-of-hdfs-ha-using-qjm</id>
    <content type="html"><![CDATA[<h2>一、前言</h2>

<p>Hadoop 1.0时代，Hadoop核心组件HDFS NameNode和MapReduce JobTracker都存在单点问题（SPOF），其中以NameNode SPOF尤为严重。HDFS作为整个Hadoop生态的基础组件，一旦NameNode发生故障，包括HDFS在内及其上层依赖组件（YARN/MapReduce/Spark/Hive/Pig/HBase等）都将无法正常工作，另外NameNode恢复过程非常耗时，尤其对大型集群，NameNode重启时间非常可观，给HDFS的可用性带来了极大的挑战，同时在使用场景上也带来明显的限制。</p>

<p>Hadoop 2.0时期，Hadoop的所有单点问题基本得到了解决。其中HDFS的高可用（High Availability，HA）在社区经过多种方案的讨论后，最终Cloudera的HA using Quorum Journal Manager（QJM）方案被社区接收，并在Hadoop 2.0版本中发布。</p>

<p>本文将从HDFS HA发展路径，架构原理，实现细节及使用过程中可能遇到的问题几个方面对HDFS NameNode高可用机制（主要是HDFS using QJM，不包含ZKFC）进行简单梳理和分析。</p>

<h2>二、HDFS HA发展路径</h2>

<p>在Hadoop 1.0时代，HDFS的架构相对简单，组件单一，主要包含NameNode，Seconday NameNode，DataNode和DFSClient四个核心组件（见图1 Hadoop 1.0中HDFS架构），其中NameNode提供元数据管理服务，Secondary NameNode以冷备的状态为NameNode分担Checkpoint工作（定期合并FsImage和Editlog），为避免出现歧义后来也称Secondary NameNode为Checkpoint Node。在这种架构下NameNode是整个系统的单点，一旦出现故障对可用性是致命的。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/hadoop1.0.png" align="center"><br />
<label class=“pic_title” align="center">图1 Hadoop 1.0中HDFS架构图</label>
</div>


<p></p>

<p>此后，社区讨论过多种HA方案来解决HDFS NameNode的单点问题，其中以2010年Facebook提出的AvatarNode方案（见图2 Facebook提出的HDFS AvatarNode架构图）较为典型。在AvatarNode方案中使用Avatar Primary NameNode，Avatar Standby NameNode及NFS配合通过共享的方式管理HDFS部分元数据EditLog。其中Avatar Primary NameNode提供读写服务，并将Editlog写入到共享存储NFS上，Avatar Standby NameNode从共享存储NFS上读取Editlog数据并回放，这样尽可能保持与Primary之间状态一致。</p>

<p>AvatarNode方案第一次使HDFS NameNode具备了热备能力，一旦Primary NameNode出现故障，Avatar Standby NameNode可以在极短时间内接管HDFS的读写请求，真正实现了HDFS的高可用。</p>

<p>但是AvatarNode这种方案并不是完美的。它的问题主要是共享存储NFS成为新的SPOF，必须保证其高可用；同时AvatarNode不具备自动Failover能力，一旦Avatar Primary NameNode出现故障，需要运维人员介入手动处理，当然Facebook这样设计有自己的考虑，这里就不再展开；另外一点，昂贵的NFS设备引入与HDFS最初构建在“inexpensive commodity hardware”设计初衷多少有些出入。</p>

<p>虽然存在问题，但是AvatarNode架构相比Hadoop 1.0已经有了本质的区别，也正是因为其在HA特性上的优秀表现，AvatarNode方案被国内外很多团队采纳和使用。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/facebookavatar.png" align="center"><br />
<label class=“pic_title” align="center">图2 Facebook提出的HDFS AvatarNode架构图</label>
</div>


<p></p>

<p>2012年初，Cloudera工程师<a href="https://github.com/toddlipcon">Todd Lipcon</a>主导设计的HA using QJM方案进入社区（见图3 HDFS HA using QJM架构图）。</p>

<p>QJM的思想最初来源于Paxos协议，摒弃了AvatarNode方案中的共享存储设备，改用多个JournalNode节点组成集群来管理和共享EditLog。与Paxos协议类似，当NameNode向JournalNode请求读写时，要求至少大多数（Majority）成功返回才认为本次请求成功。对于一个由2N+1台JournalNode组成的集群，可以容忍最多N台JournalNode节点挂掉。从这个角度来看，QJM相比AvatarNode方案具备了更强的HA能力。</p>

<p>同时QJM方案也继承了社区早前实施过的HA方案中优秀特性，通过引入Zookeeper实现的选主功能，使NameNode真正具备了自动Failover的能力。</p>

<p>对比AvatarNode，HA using QJM方案不再存在SPOF问题，High Availability特性明显提升，同时具备了自动Failover能力。正是其良好的设计和容错能力，HDFS HA using QJM方案在2012年随Hadoop 2.0正式发布，此后一直是Hadoop社区默认HDFS HA方案。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/hausingqjm.png" align="center"><br />
<label class=“pic_title” align="center">图3 HDFS HA using QJM架构图</label>
</div>


<p></p>

<p>当然除了上面提到两种典型的HDFS HA方案外，社区其实在HA特性上进行过多次尝试。</p>

<p>开始于2008年，并随Hadoop 0.21发布的BackupNode方案（参考：HADOOP-4539）；2011年通过单独分支HDFS HA Branch开发的功能非常丰富的HDFS HA方案，并在Hadoop 0.23发布（参考：HDFS-1623），其中LinuxHA和BookKeeper方案都出自这里；等等不一而足。</p>

<h2>三、HA using QJM原理</h2>

<h3>3.1 HA using QJM架构</h3>

<p>HA using Quorum Journal Manager (QJM)是当前主流HDFS HA方案，由Cloudera工程师<a href="https://github.com/toddlipcon">Todd Lipcon</a>在2012年发起，随Hadoop 2.0.3发布，此后一直被视为HDFS HA默认方案。</p>

<p>在HA using QJM方案中，涉及到的核心组件（见图3）包括：</p>

<p>Active NameNode（ANN）：在HDFS集群中，对外提供读写服务的唯一Master节点。ANN将客户端请求过来的写操作通过EditLog写入共享存储系统（即JournalNode Cluster），为Standby NameNode及时同步数据提供支持；</p>

<p>Standby NameNode（SBN）：与ANN相互形成热备，SBN及时从共享存储系统中读取EditLog数据并更新内存，以保证当前状态尽可能与ANN同步。当前在整个HDFS集群中最多一台处于Active状态，最多一台处于Standby状态；</p>

<p>JournalNode Cluster（JNs）：ANN与SBN之间共享Editlog的一致性存储系统，是HDFS NameNode高可用的核心组件。借助JournalNode集群ANN可以尽可能及时同步元数据到SBN。其中ANN采用Push模式将EditLog写入JN，SBN通过Pull模式从JN拉取数据，整个过程中JN不主动进行数据交换；</p>

<p>ZKFailoverController（ZKFC）：ZKFailoverController以独立进程运行，对NameNode主备切换进行控制，正常情况ANN和SBN分别对应各自ZKFC进程。ZKFC主要功能：NameNode健康状况检测；借助Zookeeper实现NameNode自动选主；操作NameNode进行主从切换；</p>

<p>Zookeeper（ZK）：为ZKFC实现自动选主功能提供统一协调服务。</p>

<p>需要说明的是，在HA using QJM架构下，DataNode从仅向单个NameNode进行数据交互升级到同时向ANN和SBN进行数据交互，区别是仅执行ANN下发的指令，其他逻辑未发生大变化。</p>

<h3>3.2 JournalNode Cluster实现</h3>

<p>前面提到NameNode与JournalNode Cluster进行数据读写时核心点是需要大多数JN成功返回才可认为本次请求有效。所以同Zookeeper部署类似，实际部署JournalNode Cluster时也建议奇数节点（大多数场景选择3~5个）。当然奇数节点并非强制，事实上偶数节点组成的JournalNode Cluster也能工作，但是极端的情况会存在问题，比如活锁（原理和细节可参考Paxos made simple[2]），所以不建议这么做。</p>

<p>在JournalNode Cluster中所有JN之间完全对等，不存在Primary/Secondary区别。功能上看也极其简单，Hadoop-2.7.1分支中总共2478行代码实现了完整的JournalNode功能，详细模块见图4JournalNode功能模块。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/journalnode.png" align="center"><br />
<label class=“pic_title” align="center">图4 JournalNode功能模块</label>
</div>


<p></p>

<p>JournalNode对外提供RPC和HTTP两类数据服务接口，其中JournalNodeHttpServer提供的HTTP服务，除暴露常规Metrics信息外，同时提供了被动Editlog数据同步功能（后面会详细介绍）。JournalNodeRpcServer提供的RPC Server，为NameNode向JournalNode的数据读写和状态获取请求准备了完备的RPC接口，详见QJournalProtocol.proto。Journal模块是JournalNodeRpcServer的具体实现，在Journal模块里，除了简单维护JournalNode状态信息，核心实现是抽象底层存储介质的读写操作。</p>

<p>整体上看，JournalNode模块清晰，实现简单，其中有几处非常讨巧的实现：</p>

<p>1、为了降低读写操作相互影响，Journal采用了DoubleBuffer技术管理实时过来的Editlog数据，通过DoubleBuffer可以为高速设备（内存）与低速设备（磁盘/SSD）之间建立缓存区和管道，避免数据写入被低速设备阻塞影响性能；</p>

<p>2、NameNode到JournalNode的所有数据写入请求都会直接落盘，当然写入请求的数据可以是批量，只有数据持久化完成才能认为本次请求有效和成功，这一点在数据恢复时非常关键；</p>

<p>3、与Pasox/Zookeeper类似，所有到达JournalNode的读写请求，第一件事情是合法性校验，包括EpochNum，CommitTxid等在内的状态信息，只有校验通过才能被处理，状态校验是强一致保证的基础；</p>

<p>一句话总结JournalNode是一套提供读写服务并实时持久化序列数据的有状态存储系统。</p>

<h3>3.3 Active NameNode端实现</h3>

<p>整个HA using QJM方案核心部分都集中在NameNode端，也可以认为是QJournal的Client端，这里集中了所有关于数据一致性保证和状态合理转换的主要内容。其中Active NameNode因为是写入端，所以实现逻辑也较复杂。</p>

<p>ANN按照响应客户端写请求的粒度实时顺序持久化到JournalNode，也是ANN请求JournalNode的最小粒度FSEditLogOp（HDFS写操作序列化数据）。这里首先需要权衡关于如何在JournalNode落地数据的问题：</p>

<p>1、将所有的FSEditLogOp数据落到同一个文件；<br/>
2、将每一条FSEditLogOp数据落到一个文件；<br/>
3、折中方案；</p>

<p>前面已经提到，QJournal必须保证完整事务和数据一致性，这就要求具备数据恢复的能力。如果将所有FSEditLogOp都落到一个文件，势必会带来管理成本上额外的开销，一旦出现数据不一致情况恢复大文件的代价非常高，同时同一文件累计了大量事务请求，写入失败风险非常高（后续会详细介绍）；另一种极端情况是对每一条FSEditLogOp写入一个文件，这种方式在容错和数据异常恢复会非常方便，但显然读写效率极差。所以必须对两种极端情况做折中（计算机领域内大多问题都在tradeoff）。</p>

<p>按照前面的分析，折中的唯一办法就是对连续FSEditLogOp进行分段管理。事前给每一条FSEditLogOp分配唯一且连续的事务ID称为txid，连续多个txid组成Segment，Segment持久化后对应一个EditLog文件，这样一来，任何时间JournalNode上有且仅有一个Segment处于Inprogress状态，其他均为Finalized状态。即使存在数据不一致仅需恢复Inprogress状态的Segment，Finalized Segment一定是大多数JournalNode成功写入。这样权衡可以较好解决两种极端情况的问题。</p>

<p>在ANN进程的整个生命周期里，按照不同阶段，与JournalNode交互的逻辑可以简单划分成三个部分，如图5所示Active NameNode与JournalNode之间交互的状态转换图。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/changestate.png" align="center"><br />
<label class=“pic_title” align="center">图5 Active NameNode与JournalNode之间交互的状态转换图</label>
</div>


<p></p>

<p>ANN与JournalNode在整个过程中RPC交互非常频繁，所有RPC请求均采用异步的方式，NameNode端只要收到大多数的JournalNode请求响应即认为本次请求成功，如果部分JournalNode请求失败或者超时该节点将在当前Segment内被置为异常节点。</p>

<h4>Recovery</h4>

<p>QJournal继承了Paxos一致性模中的EpochNum技术，每次做主从切换时，尝试切换为Active的NameNode从JournalNode端读取当前的EpochNum，自增1后尝试写入JournalNode。当出现多个NameNode均尝试切换为Active时会因为EpochNum问题最多只有一个成功，这从Qjournal的角度可以彻底杜绝出现Brain-Split问题。</p>

<p>NameNode操作主从切换后，首先需要确认所有JournalNode上的Editlog数据保持同步，前面提到只有最后一个Segment可能出现不一致情况，之后才能从JournalNode上拉取最新未同步的Editlog，更新内存以达到最新状态。其中保证所有JournalNode的Segment同步即是Recovery过程，整个过程如图6。</p>

<p>1、NameNode向JournalNode请求获取当前Epoch，从结果中选取出最大的Epoch；</p>

<p>2、将最大的Epoch自增1后尝试请求写入JournalNode；</p>

<p>如前述，以上两步可以在QJournal防止NameNode Brain-Split。这种竞争写入的方式可以保证任意时间最多只能存在一个NameNode有能力写入JournalNode，也是QJournal强一致的基础（关于该算法强一致性模型的形式化证明可参考Paxos made Simple[2]）。</p>

<p>3、向JournalNode请求获取最新Segment的起始事务编号（txid），具体实现时这个步骤与前一步合并，可以减少一次RPC请求；</p>

<p>4、取前一步所有返回的txid最大值segmentTxId作为本次Recovery输入进入数据恢复阶段（特殊情况是集群刚初始化时，所有的JournalNode实际上并没有Segment文件，所以数据恢复可直接跳过，判断是否需要做数据恢复的条件是有没有取到txid）：</p>

<p>（1）向JournalNode请求PrepareRecovery，参数是segmentTxId，JournalNode根据自己的实际情况返回满足起始txid为segmentTxId的Segment信息：{startTxId, endTxId, isInProgress}，分别描述了起始事务编号，最后事务编号及当前Segment是否Inprogress；</p>

<p>（2）根据SegmentRecoveryComparator算法选择所有JournalNode中最优Segment（bestSegment）准备执行Recovery；（SegmentRecoveryComparator算法：优先选择Finalized Segment，再次选择JournalNode端可见最大Epoch的Segment，最后比较endTxId最大者）</p>

<p>（3）使用前一步选出的bestSegment组装出可定位到该Segment（hostname + NamespaceInfo + segmentTxId）的URL（具体JournalNode存储Segment的位置），使用URL作为输入向JournalNode请求AcceptRecovery，当JournalNode接收到AcceptRecovery请求后，经过合法性和URL简单检查，包括startTxId，endTxid是否与本地数据存在包含关系等，通过后拿URL通过HTTP请求将目标JournalNode的Segment拉到本地，并替换本地最新一个Segment，到这里所有JournalNode上的Editlog基本一致；之所以这么说是因为最后一个Segment的数据确实是完全同步的，但是已经Finalized的Segment只能保证大多数JournalNode之间完全一致。</p>

<p>（4）请求JournalNode将最新Segment操作Finalized，最后一步操作相当于跟之前写入的数据彻底“划清界限”；</p>

<p>到这里完成了第一阶段对JournalNode数据一致性检查和修复；</p>

<p>5、NameNode进入数据更新阶段，因为有前面一致性检查和修复，另外FsImage也记录过其对应到的txid，这个阶段只要从JournalNode取回该txid之后写入的数据回放一遍，内存状态即可达到最新，这个过程称为TailEdits；当然回放完成后需要记录nextTxId，为开启ANN写操作做准备；</p>

<p>6、到这里基本具备了开放读写服务的能力，因为前面已经将所有的Segment操作了Finalized，所以开启新的Segment，即请求JournalNode初始化新Segment，并建立到JournalNode的长连接管道startLogSegment，以便数据实时写入；</p>

<p>所有的准备工作完成，NameNode正式进入Active状态，开启HDFS读写服务。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/recovery.png" align="center"><br />
<label class=“pic_title” align="center">图6 QJournal Recovery过程图示</label>
</div>


<p></p>

<h4>Log Synchronization</h4>

<p>ANN正式开启读写服务后，所有读请求在NameNode端即可完成，但是写请求需要实时同步给JournalNode，以便SBN能够及时读取并回放，以保持与Active几乎接近的状态。</p>

<p>ANN写FSEditLogOp的整体流程如图7所示：</p>

<p>1、首先在ANN处理客户端写请求的最后一个阶段，根据当前的请求类型和请求参数及当前的txid组成FSEditLogOp；</p>

<p>2、ANN端将FSEditLogOp写入到DoubleBuffer，这个阶段在NameNode整个FSNamesystem锁内，所以在任意时间仅有唯一的线程可以写入数据。可以看到DoubleBuffer技术在NameNode和JournalNode端均有使用，这里使用的目的与前面讲过的原因基本相当，一方面为低速设备（网络通道）和高速设备（内存）之间建立缓存区和管道，另一方面DoubleBuffer也可以降低锁的粒度，以到达更好的性能；</p>

<p>3、ANN处理客户端写请求的锁外，NameNode请求执行一次对FSEditLogOp的Sync操作，也就是说NameNode端写缓存是单线程操作，但是Sync可能是批量操作，但是从客户端角度来看写请求都会保证数据安全落到JouranlNode后才会返回，所以整体上看NameNode具备强一致性保证，当然整个文件系统强一致保证由多个部分共同支撑，这里不再展开。ANN端的Sync首先从DoubleBuffer里读出所有准备就绪的数据，通过RPC请求送达JournalNode，这类请求到达JournalNode端会第一时间持久化，保证数据可靠；</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/write.png" align="center"><br />
<label class=“pic_title” align="center">图7 Active NameNode写数据到JournalNode示意图</label>
</div>


<p></p>

<h4>RollEditLog</h4>

<p>前面也提到，单个Segment不可能无限大，是按照区间进行划分的，当然这个区间的划分一定不只有一条标准，默认情况下，ANN端内独立线程每间隔5min做一次检查，如果当前累计写入的FSEditLogOp超过2,000,000条操作Segment滚动，可以看到，事实上这个区间的大小可能会超出2,000,000（每间隔5min检查一次），ANN内的检查机制是为了防止SBN不工作时的补偿机制。</p>

<p>当然，请求执行RollEditLog不单单ANN端的线程，事实上SBN也会触发RollEditLog，SBN默认每1min操作执行一次EditLog回放，在回放EditLog前如果发现超过两分钟没有RollEditLog且期间有新增的FSEditLogOp，先请求ANN强制进行RollEditLog。在正常情况下，通过SBN请求执行RollEditLog是控制Segment规模的直接方法。</p>

<p>RollEditLog与前面的Recovery过程最后一步类似，首先将当前正在写入Segment操作Finalized，然后请求JournalNode初始化新Segment，建立到JournalNode的长连接管道并startLogSegment。</p>

<h3>3.4 Standby NameNode端实现</h3>

<p>SBN作为ANN的热备，需要尽可能保持与ANN状态一致，做好随时接管ANN任务的准备。当然在不损失一致性保证的前提下如果能分担ANN的部分请求处理会更好。</p>

<p>如前述，SBN为了保持与ANN状态接近甚至一致，默认每间隔1min回放一次EditLog，回放的第一步是从合适的JournalNode拉取Segment。</p>

<p>在Active NameNode端实现一节已经提过ANN写到JournalNode的Segment包含了startTxId和endTxId，所以SBN每回放完一个Segment会记录当前已经回放到的txid。这为接下来继续拉取Segment提供了起始位置，也是本次从JournalNode拉取数据的唯一输入。</p>

<p>首先，SBN根据当前的txid+1从JournalNode端获取所有包含了txid+1或者startTxId大于txid+1的Segment列表；</p>

<p>之后，根据取回Segment集合做简单过滤和排序，过滤是为了找到更合适的Segment，比如相同txid集合的Segment，回放Finalized更安全；保留多个Segment并排序是为了更好的容错；</p>

<p>最后，按照已排好序Segment，逐个FSEditLogOp尝试取Segment内容，按照FSEditLogOp粒度读取并回放，直到改Segment回放完成；</p>

<p>当前实现中，SBN仅回放Finalized状态的Segment。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/ha/read.png" align="center"><br />
<label class=“pic_title” align="center">图8 Standby NameNode从JournalNode读数据示意图</label>
</div>


<p></p>

<h3>3.5 数据一致性保证</h3>

<p>从前面的分析来看，QJM本质上是一种极其简化版的Paxos协议，所以基本具备了Paxos优势。如果按照分布式系统经典理论CAP来评估，QJM是一种强一致性、高可用的去中心化分布式协议。</p>

<p>高可用：QJM的高可用特性其实是完全继承自Paxos，但在具体实现中为实现强一致性实际上牺牲了少部分高可用性。Paxos中Quorum理论在QJM依然稳定，也就是对于JournalNode Cluster，最多可以容忍小于一半的节点故障，系统仍可正常运行。但是如前面提到为了实现强一致性，QJM放大了故障范围（只要出现一次请求响应失败或者超时即标记该JournalNode在当前Segment范围内失效outOfSync），而且对于故障完全没有恢复能力，虽然通过限定Segment范围尽力补救故障影响范围，但是不可否认因为故障被放大，且没有恢复机制，不可用的风险同故障范围被线性放大。</p>

<p>高度去中心化：前面的流程分析过程不难看出，JournalNode整个生命周期，仅初始化阶段由ANN触发过一次选主以及JournalNode间的交互过程，其他阶段JournalNode之间完全对等，完全无中心化节点，这样也就不存在SPOF的问题，所以也具备了非常好Partition Tolerance特性。</p>

<p>强一致性：</p>

<p>1、强一致性是Paxos及各衍生系统最明显的优势特性，QJM的一致性思想主要来源于Paxos，另外为了达到更好的性能，在Pasox基础上又放松了很多限制条件。比如相比Paxos仅需要竞争一次，完成后继承结果，不需要为强一致每一轮数据读写都先竞选影响性能，从这个角度看，QJM与Raft也有相似的特征（是不是存在更本质一致性算法）；当然，QJournal放松诸多限制条件跟HDFS的使用场景强相关，比如最多只存在两个QJournal Client，竞争发生的条件非常严苛，即使极端情况产生竞争也能够在一轮竞选完成；</p>

<p>2、延后读保证数据强一致，SBN当前仅读取和回放Finalized状态Segment，可以保证Finalized数据强一致，借鉴Paxos思想在数据恢复阶段可以做到Inprogress状态数据强一致，同时QJM采用了一种尽力而为的恢复机制，对不确定状态定义了统一恢复策略；</p>

<p>3、全局有序和递增的Epoch序号，任意时间仅一个竞争者（QJournal Client即NameNode）胜出，保证写入端的唯一性和合法性。</p>

<h2>四、QJM的问题及解决思路</h2>

<p>虽然HA using QJM方案作为HDFS默认HA方案已经在社区稳定运行了超过5年时间，但实际上还是存在一些问题：</p>

<p>1、Paxos算法在出现竞争时，收敛速度慢，甚至可能出现活锁的情况，QJM并没有针对这种问题进行过优化。极端情况下如果出现两个NameNode同时竞争写入，也可能陷入僵局，虽然不至于污染数据（Brain-Split），但是存在永远竞争陷入僵局的可能。</p>

<p>2、QJM放大了故障范围，且在Segment周期里没有任何恢复机制，虽然通过限制Segment大小进行了补偿，但是风险被线性放大，尤其对JournalNode小集群及配置Segment较大事务区间；</p>

<p>3、在线升级JournalNode让QJournal可用性风险成倍放大，原因同问题2；</p>

<p>4、QJM对JournalNode在单Segment故障没有恢复机制，ANN一旦遇到写入失败，只能操作主从切换甚至重启，对规模较大的Hadoop集群，重启NameNode成本非常高；</p>

<p>针对上述问题，实际上业界已有一些对应的解决办法，不过需要强调的是所有解决办法都在尽力权衡CAP：</p>

<p>1、对于Paxos在竞争情况下收敛慢和活锁问题，在HDFS场景里出现的概率极小，而且最多只有两个竞争者，且两个竞争者同时出现的唯一可能是误操作尝试将两个NameNode均切换成ANN，这种问题应该尽可能在运维中避免；从技术的角度看，虽然可以通过Leader选择，加快收敛速度和避免活锁，但是在HDFS场景下为解决极端情况牺牲可用性是否有必要值得商榷；</p>

<p>2、JournalNode故障快速恢复完全可以借鉴RollEditLog的办法，增加触发RollEditLog的条件，同时需要考虑Journal出现故障后频繁RollEditLog带来的性能损失；</p>

<p>3、实际场景里，最需要解决的还是NameNode因为Journal写入失败造成进程退出的问题，可借鉴的方案也很多，这里列出两种代价较小方案供参考：
（1）所有Journal写入失败，强制NameNode退出竞争者，进入Standby状态；
（2）延长请求JournalNode超时时间，用损失极端情况下性能损失换取NameNode重启成本；</p>

<h2>五、总结</h2>

<p>本文从HDFS HA的发展过程，各种方案设计背后的考虑，以及社区选择的默认HA using QJM方案原理和其中存在的问题及解决思路简单分析。<br/>
通过QJM原理梳理和实现细节分析，可以深入理解HDFS HA现状和存在问题，为后续运维甚至优化改进积累经验。</p>

<h2>六、参考</h2>

<p>[1] <a href="https://hadoop.apache.org">https://hadoop.apache.org</a><br/>
[2] <a href="https://www.microsoft.com/en-us/research/publication/paxos-made-simple/">https://www.microsoft.com/en-us/research/publication/paxos-made-simple/</a><br/>
[3] <a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a><br/>
[4] <a href="https://issues.apache.org/jira/secure/attachment/12435811/AvatarNodeDescription.txt">https://issues.apache.org/jira/secure/attachment/12435811/AvatarNodeDescription.txt</a><br/>
[5] <a href="http://www.cloudera.com/blog/2009/07/hadoop-ha-configuration/">http://www.cloudera.com/blog/2009/07/hadoop-ha-configuration/</a><br/>
[6] <a href="http://www.cloudera.com/blog/2012/03/high-availability-for-the-hadoop-distributed-file-system-hdfs/">http://www.cloudera.com/blog/2012/03/high-availability-for-the-hadoop-distributed-file-system-hdfs/</a><br/>
[7] <a href="http://zookeeper.apache.org/bookkeeper/">http://zookeeper.apache.org/bookkeeper/</a><br/>
[8] <a href="https://github.com/apache/hadoop/tree/branch-2.7.1">https://github.com/apache/hadoop/tree/branch-2.7.1</a><br/>
[9] <a href="https://issues.apache.org/jira/browse/HDFS-3077">https://issues.apache.org/jira/browse/HDFS-3077</a><br/>
[10] <a href="https://issues.apache.org/jira/browse/HDFS-1623">https://issues.apache.org/jira/browse/HDFS-1623</a><br/>
[11] <a href="https://issues.apache.org/jira/browse/HADOOP-4539">https://issues.apache.org/jira/browse/HADOOP-4539</a><br/>
[12] <a href="https://issues.apache.org/jira/browse/HDFS-976">https://issues.apache.org/jira/browse/HDFS-976</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS NameNode重启优化]]></title>
    <link href="http://hexiaoqiao.github.io/blog/2017/02/12/namenode-restart-optimization/"/>
    <updated>2017-02-12T10:45:00+08:00</updated>
    <id>http://hexiaoqiao.github.io/blog/2017/02/12/namenode-restart-optimization</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>在Hadoop集群整个生命周期里，由于调整参数、Patch、升级等多种场景需要频繁操作NameNode重启，不论采用何种架构，重启期间集群整体存在可用性和可靠性的风险，所以优化NameNode重启非常关键。</p>

<p>本文基于<a href="https://github.com/apache/hadoop/tree/branch-2">Hadoop-2.x</a>和<a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">HA with QJM</a>社区架构和系统设计（如图1所示），通过梳理NameNode重启流程，并在此基础上，阐述对NameNode重启优化实践。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/restartnn/qjm.png" align="center"><br />
<label class=“pic_title” align="center">图1 HDFS HA with QJM架构图示</label>
</div>


<p></p>

<h2>二、NameNode重启流程</h2>

<p>在HDFS的整个运行期里，所有元数据均在NameNode的内存集中管理，但是由于内存易失特性，一旦出现进程退出、宕机等异常情况，所有元数据都会丢失，给整个系统的数据安全会造成不可恢复的灾难。为了更好的容错能力，NameNode会周期进行Checkpoint，将其中的一部分元数据（文件系统的目录树Namespace）刷到持久化设备上，即二进制文件FSImage，这样的话即使NameNode出现异常也能从持久化设备上恢复元数据，保证了数据的安全可靠。</p>

<p>但是仅周期进行Checkpoint仍然无法保证所有数据的可靠，如前次Checkpoint之后写入的数据依然存在丢失的问题，所以将两次Checkpoint之间对Namespace写操作实时写入EditLog文件，通过这种方式可以保证HDFS元数据的绝对安全可靠。</p>

<p>事实上，除Namespace外，NameNode还管理非常重要的元数据BlocksMap，描述数据块Block与DataNode节点之间的对应关系。NameNode并没有对这部分元数据同样操作持久化，原因是每个DataNode已经持有属于自己管理的Block集合，将所有DataNode的Block集合汇总后即可构造出完整BlocksMap。</p>

<p>HA with QJM架构下，NameNode的整个重启过程中始终以SBN（StandbyNameNode）角色完成。与前述流程对应，启动过程分以下几个阶段：<br/>
0、加载FSImage；<br/>
1、回放EditLog；<br/>
2、执行Checkpoint；（非必须步骤，结合实际情况和参数确定，后续详述）<br/>
3、收集所有DataNode的注册和数据块汇报；</p>

<p>默认情况下，NameNode会保存两个FSImage文件，与此对应，也会保存对应两次Checkpoint之后的所有EditLog文件。一般来说，NameNode重启后，通过对FSImage文件名称判断，选择加载最新的FSImage文件及回放该Checkpoint之后生成的所有EditLog，完成后根据加载的EditLog中操作条目数及距上次Checkpoint时间间隔（后续详述）确定是否需要执行Checkpoint，之后进入等待所有DataNode注册和元数据汇报阶段，当这部分数据收集完成后，NameNode的重启流程结束。</p>

<p>从线上NameNode历次重启时间数据看，各阶段耗时占比基本接近如图2所示。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/restartnn/namenodetime.png" align="center"><br />
<label class=“pic_title” align="center">图2 NameNode重启各阶段耗时占比</label>
</div>


<p></p>

<p>经过优化，在元数据总量540M（目录树240M，数据块300M），超过4K规模的集群上重启NameNode总时间~35min，其中加载FSImage耗时~15min，秒级回放EditLog，数据块汇报耗时~20min，基本能够满足生产环境的需求。</p>

<h3>2.1 加载FSImage</h3>

<p>如前述，FSImage文件记录了HDFS整个目录树Namespace相关的元数据。从<a href="https://github.com/apache/hadoop/tree/branch-2.4.0">Hadoop-2.4.0</a>起，FSImage开始采用<a href="https://developers.google.com/protocol-buffers/">Google Protobuf</a>编码格式描述（<a href="https://issues.apache.org/jira/browse/HDFS-5698">HDFS-5698</a>），详细描述文件见<a href="https://github.com/apache/hadoop/blob/branch-2.4.0/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/fsimage.proto">fsimage.proto</a>。根据描述文件和实现逻辑，FSImage文件格式如图3所示。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/restartnn/fsimage.png" align="center"><br />
<label class=“pic_title” align="center">图3 FSImage文件格式</label>
</div>


<p></p>

<p>从fsimage.proto和FSImage文件存储格式容易看到，除了必要的文件头部校验（MAGIC）和尾部文件索引（FILESUMMARY）外，主要包含以下核心数据：</p>

<p>（0）NS_INFO（NameSystemSection）：记录HDFS文件系统的全局信息，包括NameSystem的ID，当前已经分配出去的最大BlockID以及TransactionId等信息；<br/>
（1）INODE（INodeSection）：整个目录树所有节点数据，包括INodeFile/INodeDirectory/INodeSymlink等所有类型节点的属性数据，其中记录了如节点id，节点名称，访问权限，创建和访问时间等等信息；<br/>
（2）INODE_DIR（INodeDirectorySection）：整个目录树中所有节点之间的父子关系，配合INODE可构建完整的目录树；<br/>
（3）FILES_UNDERCONSTRUCTION（FilesUnderConstructionSection）：尚未完成写入的文件集合，主要为重启时重建Lease集合；<br/>
（4）SNAPSHOT（SnapshotSection）：记录Snapshot数据，快照是Hadoop 2.1.0引入的新特性，用于数据备份、回滚，以防止因用户误操作导致集群出现数据问题；<br/>
（5）SNAPSHOT_DIFF（SnapshotDiffSection）：执行快照操作的目录/文件的Diff集合数据，与SNAPSHOT一起构建较完整的快照管理能力；<br/>
（6）INODE_REFERENCE（INodeReferenceSection）：当目录/文件被操作处于快照，且该目录/文件被重命名后，会存在多条访问路径，INodeReference就是为了解决该问题；<br/>
（7）SECRET_MANAGER（SecretManagerSection）：记录DelegationKey和DelegationToken数据，根据DelegationKey及由DelegationToken构造出的DelegationTokenIdentifier方便进一步计算密码，以上数据可以完善所有合法Token集合；<br/>
（8）CACHE_MANAGER（CacheManagerSection）：集中式缓存特性全局信息，集中式缓存特性是Hadoop-2.3.0为提升数据读性能引入的新特性；<br/>
（9）STRING_TABLE（StringTableSection）：字符串到id的映射表，维护目录/文件的Permission字符到ID的映射，节省存储空间；</p>

<p>NameNode执行Checkpoint时，遵循Protobuf定义及上述文件格式描述，重启加载FSImage时，同样按照Protobuf定义的格式从文件流中读出相应数据构建整个目录树Namespace及其他元数据。将FSImage文件从持久化设备加载到内存并构建出目录树结构后，实际上并没有完全恢复元数据到最新状态，因为每次Checkpoint之后还可能存在大量HDFS写操作。</p>

<h3>2.2 回放EditLog</h3>

<p>NameNode在响应客户端的写请求前，会首先更新内存相关元数据，然后再把这些操作记录在EditLog文件中，可以看到内存状态实际上要比EditLog数据更及时。</p>

<p>记录在EditLog之中的每个操作又称为一个事务，对应一个整数形式的事务编号。在当前实现中多个事务组成一个Segment，生成独立的EditLog文件，其中文件名称标记了起止的事务编号，正在写入的EditLog文件仅标记起始事务编号。EditLog文件的格式非常简单，没再通过Google Protobuf描述，文件格式如图4所示。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/restartnn/editlog.png" align="center"><br />
<label class=“pic_title” align="center">图4 EditLog文件格式</label>
</div>


<p></p>

<p>一个完整的EditLog文件包括四个部分内容，分别是：<br/>
（0）LAYOUTVERSION：版本信息；<br/>
（1）OP_START_LOG_SEGMENT：标识文件开始；<br/>
（2）RECORD：顺序逐个记录HDFS写操作的事务内容；<br/>
（3）OP_END_LOG_SEGMENT：标记文件结束；</p>

<p>NameNode加载FSImage完成后，即开始对该FSImage文件之后（通过比较FSImage文件名称中包含的事务编号与EditLog文件名称的起始事务编号大小确定）生成的所有EditLog严格按照事务编号从小到大逐个遵循上述的格式进行每一个HDFS写操作事务回放。</p>

<p>NameNode加载完所有必需的EditLog文件数据后，内存中的目录树即恢复到了最新状态。</p>

<h3>2.3 DataNode注册汇报</h3>

<p>经过前面两个步骤，主要的元数据被构建，HDFS的整个目录树被完整建立，但是并没有掌握从数据块Block与DataNode之间的对应关系BlocksMap，甚至对DataNode的情况都不掌握，所以需要等待DataNode注册，并完成对从DataNode汇报上来的数据块汇总。待汇总的数据量达到预设比例（dfs.namenode.safemode.threshold-pct）后退出Safemode。</p>

<p>NameNode重启经过加载FSImage和回放EditLog后，所有DataNode不管进程是否发生过重启，都必须经过以下两个步骤：<br/>
（0）DataNode重新注册RegisterDataNode；<br/>
（1）DataNode汇报所有数据块BlockReport；</p>

<p>对于节点规模较大和元数据量较大的集群，这个阶段的耗时会非常可观。主要有三点原因：<br/>
（0）处理BlockReport的逻辑比较复杂，相对其他RPC操作耗时较长。图5对比了BlockReport和AddBlock两种不同RPC的处理时间，尽管AddBlock操作也相对复杂，但是对比来看，BlockReport的处理时间显著高于AddBlock处理时间；<br/>
（1）NameNode对每一个BlockReport的RPC请求处理都需要持有全局锁，也就是说对于BlockReport类型RPC请求实际上是串行处理；<br/>
（2）NameNode重启时所有DataNode集中在同一时间段进行BlockReport请求；</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/restartnn/rpcprocesstime.png" align="center"><br />
<label class=“pic_title” align="center">图5 BlockReport和AddBlock两个RPC处理时间对比</label></div>


<p></p>

<p>前文<a href="http://hexiaoqiao.github.io/blog/2016/07/06/namenode-memory-overview/">NameNode内存全景</a>中详细描述过Block在NameNode元数据中的关键作用及与Namespace/DataNode/BlocksMap的复杂关系，从中也可以看出，每个新增Block需要维护多个关系，更何况重启过程中所有Block都需要建立同样复杂关系，所以耗时相对较高。</p>

<h2>三、重启优化</h2>

<p>根据前面对NameNode重启过程的简单梳理，在各个阶段可以适当的实施优化以加快NameNode重启过程。</p>

<p>0、<a href="https://issues.apache.org/jira/browse/HDFS-7097">HDFS-7097</a> 解决重启过程中SBN执行Checkpoint时不能处理BlockReport请求的问题；</p>

<p>Fix：<a href="https://issues.apache.org/jira/browse/HDFS/fixforversion/12327584">2.7.0</a><br/>
Hadoop-2.7.0版本前，SBN（StandbyNameNode）在执行Checkpoint操作前会先获得全局读写锁fsLock，在此期间，BlockReport请求由于不能获得全局写锁会持续处于等待状态，直到Checkpoint完成后释放了fsLock锁后才能继续。NameNode重启的第三个阶段，同样存在这种情况。而且对于规模较大的集群，每次Checkpoint时间在分钟级别，对整个重启过程影响非常大。实际上，Checkpoint是对目录树的持久化操作，并不涉及BlocksMap数据结构，所以Checkpoint期间是可以让BlockReport请求直接通过，这样可以节省期间BlockReport排队等待带来的时间开销，<a href="https://issues.apache.org/jira/browse/HDFS-7097">HDFS-7097</a>正是将锁粒度放小解决了Checkpoint过程不能处理BlockReport类型RPC请求的问题。</p>

<p>与<a href="https://issues.apache.org/jira/browse/HDFS-7097">HDFS-7097</a>相对，另一种思路也值得借鉴，就是重启过程尽可能避免出现Checkpoint。触发Checkpoint有两种情况：时间周期或HDFS写操作事务数，分别通过参数dfs.namenode.checkpoint.period和dfs.namenode.checkpoint.txns控制，默认值分别是3600s和1,000,000，即默认情况下一个小时或者写操作的事务数超过1,000,000触发一次Checkpoint。为了避免在重启过程中频繁执行Checkpoint，可以适当调大dfs.namenode.checkpoint.txns，建议值10,000,000 ~ 20,000,000，带来的影响是EditLog文件累计的个数会稍有增加。从实践经验上看，对一个有亿级别元数据量的NameNode，回放一个EditLog文件（默认1,000,000写操作事务）时间在秒级，但是执行一次Checkpoint时间通常在分钟级别，综合权衡减少Checkpoint次数和增加EditLog文件数收益比较明显。</p>

<p>1、<a href="https://issues.apache.org/jira/browse/HDFS-6763">HDFS-6763</a> 解决SBN每间隔1min全局计算和验证Quota值导致进程Hang住数秒的问题；</p>

<p>Fix：<a href="https://issues.apache.org/jira/browse/HDFS/fixforversion/12329057">2.8.0</a><br/>
ANN（ActiveNameNode）将HDFS写操作实时写入JN的EditLog文件，为同步数据，SBN默认间隔1min从JN拉取一次EditLog文件并进行回放，完成后执行全局Quota检查和计算，当Namespace规模变大后，全局计算和检查Quota会非常耗时，在此期间，整个SBN的Namenode进程会被Hang住，以至于包括DN心跳和BlockReport在内的所有RPC请求都不能及时处理。NameNode重启过程中这个问题影响突出。<br/>
实际上，SBN在EditLog Tailer阶段计算和检查Quota完全没有必要，HDFS-6763将这段处理逻辑后移到主从切换时进行，解决SBN进程间隔1min被Hang住的问题。<br/>
从优化效果上看，对一个拥有接近五亿元数据量，其中两亿数据块的NameNode，优化前数据块汇报阶段耗时~30min，其中触发超过20次由于计算和检查Quota导致进程Hang住~20s的情况，整个BlockReport阶段存在超过5min无效时间开销，优化后可到~25min。</p>

<p>2、<a href="https://issues.apache.org/jira/browse/HDFS-7980">HDFS-7980</a> 简化首次BlockReport处理逻辑优化重启时间；</p>

<p>Fix：<a href="https://issues.apache.org/jira/browse/HDFS/fixforversion/12331979">2.7.1</a><br/>
NameNode加载完元数据后，所有DataNode尝试开始进行数据块汇报，如果汇报的数据块相关元数据还没有加载，先暂存消息队列，当NameNode完成加载相关元数据后，再处理该消息队列。对第一次块汇报的处理比较特别（NameNode重启后，所有DataNode的BlockReport都会被标记成首次数据块汇报），为提高处理速度，仅验证块是否损坏，之后判断块状态是否为FINALIZED，若是建立数据块与DataNode的映射关系，建立与目录树中文件的关联关系，其他信息一概暂不处理。对于非初次数据块汇报，处理逻辑要复杂很多，对报告的每个数据块，不仅检查是否损坏，是否为FINALIZED状态，还会检查是否无效，是否需要删除，是否为UC状态等等；验证通过后建立数据块与DataNode的映射关系，建立与目录树中文件的关联关系。</p>

<p>初次数据块汇报的处理逻辑独立出来，主要原因有两方面：<br/>
（0）加快NameNode的启动时间；测试数据显示含~500M元数据的NameNode在处理800K个数据块的初次块汇报的处理时间比正常块汇报的处理时间可降低一个数量级；<br/>
（1）启动过程中，不提供正常读写服务，所以只要确保正常数据（整个Namespace和所有FINALIZED状态Blocks）无误，无效和冗余数据处理完全可以延后到IBR（IncrementalBlockReport）或下次BR（BlockReport）；</p>

<p>这本来是非常合理和正常的设计逻辑，但是实现时NameNode在判断是否为首次数据块块汇报的逻辑一直存在问题，导致这段非常好的改进点逻辑实际上长期并未真正执行到，直到<a href="https://issues.apache.org/jira/browse/HDFS-7980">HDFS-7980</a>在Hadoop-2.7.1修复该问题。<a href="https://issues.apache.org/jira/browse/HDFS-7980">HDFS-7980</a>的优化效果非常明显，测试显示，对含80K Blocks的BlockReport RPC请求的处理时间从~500ms可优化到~100ms，从重启期整个BlockReport阶段看，在超过600M元数据，其中300M数据块的NameNode显示该阶段从~50min优化到~25min。</p>

<p>3、<a href="https://issues.apache.org/jira/browse/HDFS-7503">HDFS-7503</a> 解决重启前大删除操作会造成重启后锁内写日志降低处理能力；</p>

<p>Fix：<a href="https://issues.apache.org/jira/browse/HDFS/fixforversion/12327584">2.7.0</a><br/>
若NameNode重启前产生过大删除操作，当NameNode加载完FSImage并回放了所有EditLog构建起最新目录树结构后，在处理DataNode的BlockReport时，会发现有大量Block不属于任何文件，Hadoop-2.7.0版本前，对于这类情况的输出日志逻辑在全局锁内，由于存在大量IO操作的耗时，会严重拉长处理BlockReport的处理时间，影响NameNode重启时间。<a href="https://issues.apache.org/jira/browse/HDFS-7503">HDFS-7503</a>的解决办法非常简单，把日志输出逻辑移出全局锁外。线上效果上看对同类场景优化比较明显，不过如果重启前不触发大的删除操作影响不大。</p>

<p>4、防止热备节点SBN（StandbyNameNode）/冷备节点SNN（SecondaryNameNode）长时间未正常运行堆积大量Editlog拖慢NameNode重启时间；</p>

<p>不论选择HA热备方案SBN（StandbyNameNode）还是冷备方案SNN（SecondaryNameNode）架构，执行Checkpoint的逻辑几乎一致，如图6所示。如果SBN/SNN服务长时间未正常运行，Checkpoint不能按照预期执行，这样会积压大量EditLog。积压的EditLog文件越多，重启NameNode需要加载EditLog时间越长。所以尽可能避免出现SNN/SBN长时间未正常服务的状态。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/restartnn/checkpoint.png" align="center"><br />
<label class=“pic_title” align="center">图6 Checkpoint流程</label>
</div>


<p></p>

<p>在一个有500M元数据的NameNode上测试加载一个200K次HDFS事务操作的EditLog文件耗时~5s，按照默认2min的EditLog滚动周期，如果一周时间SBN/SNN未能正常工作，则会累积~5K个EditLog文件，此后一旦发生NameNode重启，仅加载EditLog文件的时间就需要~7h，也就是整个集群存在超过7h不可用风险，所以切记要保证SBN/SNN不能长时间故障。</p>

<p>5、<a href="https://issues.apache.org/jira/browse/HDFS-6425">HDFS-6425</a> <a href="https://issues.apache.org/jira/browse/HDFS-6772">HDFS-6772</a> NameNode重启后DataNode快速退出blockContentsStale状态防止PostponedMisreplicatedBlocks过大影响对其他RPC请求的处理能力；</p>

<p>Fix: <a href="https://issues.apache.org/jira/browse/HDFS/fixforversion/12327181">2.6.0</a>, <a href="https://issues.apache.org/jira/browse/HDFS/fixforversion/12327584">2.7.0</a><br/>
当集群中大量数据块的实际存储副本个数超过副本数时（跨机房架构下这种情况比较常见），NameNode重启后会迅速填充到PostponedMisreplicatedBlocks，直到相关数据块所在的所有DataNode汇报完成且退出Stale状态后才能被清理。如果PostponedMisreplicatedBlocks数据量较大，每次全遍历需要消耗大量时间，且整个过程也要持有全局锁，严重影响处理BlockReport的性能，<a href="https://issues.apache.org/jira/browse/HDFS-6425">HDFS-6425</a>和<a href="https://issues.apache.org/jira/browse/HDFS-6772">HDFS-6772</a>分别将可能在BlockReport逻辑内部遍历非常大的数据结构PostponedMisreplicatedBlocks优化到异步执行，并在NameNode重启后让DataNode快速退出blockContentsStale状态避免PostponedMisreplicatedBlocks过大入手优化重启效率。</p>

<p>6、降低BlockReport时数据规模；</p>

<p>NameNode处理BlockReport的效率低主要原因还是每次BlockReport所带的Block规模过大造成，所以可以通过调整Block数量阈值，将一次BlockReport分成多盘分别汇报，以提高NameNode对BlockReport的处理效率。可参考的参数为：dfs.blockreport.split.threshold，默认值1,000,000，即当DataNode本地的Block个数超过1,000,000时才会分盘进行汇报，建议将该参数适当调小，具体数值可结合NameNode的处理BlockReport时间及集群中所有DataNode管理的Block量分布确定。</p>

<p>7、重启完成后对比检查数据块上报情况；</p>

<p>前面提到NameNode汇总DataNode上报的数据块量达到预设比例（dfs.namenode.safemode.threshold-pct）后就会退出Safemode，一般情况下，当NameNode退出Safemode后，我们认为已经具备提供正常服务的条件。但是对规模较大的集群，按照这种默认策略及时执行主从切换后，容易出现短时间丢块的问题。考虑在200M数据块的集群，默认配置项dfs.namenode.safemode.threshold-pct=0.999，也就是当NameNode收集到200M*0.999=199.8M数据块后即可退出Safemode，此时实际上还有200K数据块没有上报，如果强行执行主从切换，会出现大量的丢块问题，直到数据块汇报完成。应对的办法比较简单，尝试调大dfs.namenode.safemode.threshold-pct到1，这样只有所有数据块上报后才会退出Safemode。但是这种办法一样不能保证万无一失，如果启动过程中有DataNode汇报完数据块后进程挂掉，同样存在短时间丢失数据的问题，因为NameNode汇总上报数据块时并不检查副本数，所以更稳妥的解决办法是利用主从NameNode的JMX数据对比所有DataNode当前汇报数据块量的差异，当差异都较小后再执行主从切换可以保证不发生上述问题。</p>

<p>8、其他；</p>

<p>除了优化NameNode重启时间，实际运维中还会遇到需要滚动重启集群所有节点或者一次性重启整集群的情况，不恰当的重启方式也会严重影响服务的恢复时间，所以合理控制重启的节奏或选择合适的重启方式尤为关键，<a href="http://hexiaoqiao.github.io/blog/2016/07/07/restart-cluster-ways-when-upgrade/">HDFS集群启动方式分析</a>一文对集群重启方式进行了详细的阐述，这里就不再展开。</p>

<p>经过多次优化调整，从线上NameNode历次的重启时间监控指标上看，收益非常明显，图7截取了其中几次NameNode重启时元数据量及重启时间开销对比，图中直观显示在500M元数据量级下，重启时间从~4000s优化到~2000s。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/restartnn/time.png" align="center"><br />
<label class=“pic_title” align="center">图7 NameNode重启时间对比</label>
</div>


<p></p>

<p>这里罗列了一小部分实践过程中可以有效优化重启NameNode时间或者重启全集群的点，其中包括了社区成熟Patch和相关参数优化，虽然实现逻辑都很小，但是实践收益非常明显。当然除了上述提到，NameNode重启还有很多可以优化的地方，比如优化FSImage格式，并行加载等等，社区也在持续关注和优化，部分讨论的思路也值得关注、借鉴和参考。</p>

<h2>四、总结</h2>

<p>NameNode重启甚至全集群重启在整个Hadoop集群的生命周期内是比较频繁的运维操作，优化重启时间可以极大提升运维效率，避免可能存在的风险。本文通过分析NameNode启动流程，并结合实践过程简单罗列了几个供参考的有效优化点，借此希望能给实践过程提供可优化的方向和思路。</p>

<h2>五、参考</h2>

<p>[1] NameNode内存全景. <a href="http://tech.meituan.com/namenode.html">http://tech.meituan.com/namenode.html</a><br/>
[2] NameNode内存详解. <a href="http://tech.meituan.com/namenode-memory-detail.html">http://tech.meituan.com/namenode-memory-detail.html</a><br/>
[3] Apache Hadoop. <a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a><br/>
[4] Hadoop Source. <a href="https://github.com/apache/hadoop">https://github.com/apache/hadoop</a><br/>
[5] HDFS Issues. <a href="https://issues.apache.org/jira/browse/HDFS">https://issues.apache.org/jira/browse/HDFS</a><br/>
[6] Cloudera Blog. <a href="http://blog.cloudera.com/">http://blog.cloudera.com/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NameNode RepicationMonitor异常追查]]></title>
    <link href="http://hexiaoqiao.github.io/blog/2016/09/13/namenode-repicationmonitor-exception-trace/"/>
    <updated>2016-09-13T10:45:00+08:00</updated>
    <id>http://hexiaoqiao.github.io/blog/2016/09/13/namenode-repicationmonitor-exception-trace</id>
    <content type="html"><![CDATA[<p>集群版本从2.4.1升级到2.7.1之后，出现了一个诡异的问题，虽然没有影响到线上正常读写服务，但是潜在的问题还是比较严重，经过追查彻底解决，这里简单整理追查过程。</p>

<h2>一、问题描述</h2>

<p>异常初次出现时收集到的集群异常表现信息有两条：</p>

<p>1、两个关键数据结构持续堆积，监控显示UnderReplicatedBlocks和PendingDeletionBlocks表现明显。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/monitor/underreplicatedblocks.png" alt="NameNode UnderReplicatedBlocks数据结构变化趋势" align="center"><br />
<label class=“pic_title” align="center">图1 NameNode UnderReplicatedBlocks数据结构变化趋势</label>
</div>


<p></p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/monitor/pendingblocks.png" alt="NameNode PendingBlocks数据结构变化趋势" align="center"><br />
<label class=“pic_title” align="center">图2 NameNode PendingBlocks数据结构变化趋势</label>
</div>


<p></p>

<p><em>说明：没有找到异常同一时间段的监控图，可将上图时间点简单匹配，基本不影响后续的分析。</em></p>

<p>2、从NameNode的jstack获得信息ReplicationMonitor线程在长期执行chooseRandom函数；<br/>
<figure class='code'><figcaption><span>namenode.jstack </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="ni">&amp;ldquo;</span>org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@254e0df1<span class="ni">&amp;rdquo;</span> daemon prio=10 tid=0x00007f59b4364800 nid=0xa7d9 runnable [0x00007f2baf40b000]
</span><span class='line'>   java.lang.Thread.State: RUNNABLE
</span><span class='line'>        at java.util.AbstractCollection.toArray(AbstractCollection.java:195)
</span><span class='line'>        at java.lang.String.split(String.java:2311)
</span><span class='line'>        at org.apache.hadoop.net.NetworkTopology$InnerNode.getLoc(NetworkTopology.java:282)
</span><span class='line'>        at org.apache.hadoop.net.NetworkTopology$InnerNode.getLoc(NetworkTopology.java:292)
</span><span class='line'>        at org.apache.hadoop.net.NetworkTopology$InnerNode.access$000(NetworkTopology.java:82)
</span><span class='line'>        at org.apache.hadoop.net.NetworkTopology.getNode(NetworkTopology.java:539)
</span><span class='line'>        at org.apache.hadoop.net.NetworkTopology.countNumOfAvailableNodes(NetworkTopology.java:775)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:707)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:383)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:432)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:225)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:120)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3783)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3748)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1408)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1314)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3719)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3671)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span></code></pre></td></tr></table></div></figure></p>

<p>由于线上环境的日志级别为INFO，而ReplicationMonitor中INFO级别之上的日志非常少，从中几乎不能获取到任何有用信息；</p>

<p>异常出现场景：<br/>
1、坏盘、DataNode Decommision或进程异常退出，但不能稳定复现；<br/>
2、外部环境无任何变化和异常，正常读写服务期偶发。</p>

<h2>二、追查过程</h2>

<h3>2.1 处理线上问题</h3>

<p>ReplicationMonitor线程运行异常，造成数据块的副本不能及时补充，如果异常长期存在，极有可能出现丢数据的情况，在没有其他信息辅助解决的情况下，唯一的办法就是重启NameNode（传说中的“三大招”之一），好在HA架构的支持，不至于影响到正常数据生产。</p>

<h3>2.2 日志</h3>

<p>缺少日志，不能定位问题出现的场景，所以首先需要在关键路径上留下必要的信息，方便追查。由于ReplicationMonitor属于独立线程，合理的日志量输出不至于影响服务性能，经过多次调整基本确定需要收集的日志信息：</p>

<p>1、根据NameNode多次jstack信息，怀疑chooseRandom时不停计算countNumOfAvailableNodes，可能存在死循环，尝试输出两类信息：<br/>
（1）ReplicationMonitor当前处理的整体参数及正在处理的Block；
<figure class='code'><figcaption><span>BlockManager.java <a href="https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L3662">https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L3662</a> github </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="n">numlive</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">numlive</span><span class="o">);</span>
</span><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="n">blockToProcess</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">blocksToProcess</span><span class="o">);</span>
</span><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="n">nodeToProcess</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">nodesToProcess</span><span class="o">);</span>
</span><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="n">blocksInvalidateWorkPct</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="k">this</span><span class="o">.</span><span class="na">blocksInvalidateWorkPct</span><span class="o">);</span>
</span><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="n">workFound</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">workFound</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>（2）chooseRandom逻辑中循环体内（调用了countNumOfAvailableNodes）运行超过1min输出该函数入口的所有参数；问题复现后，日志并没有输出，说明异常并不在chooseRandom逻辑本身；</p>

<p>2、结合NameNode的jstack信息并跟进实现逻辑时发现NetworkTopology.InnerNode#getLoc(String loc)的实现存在性能问题：<br/>
<figure class='code'><figcaption><span>NetworkTopology.java <a href="https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java#L274">https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java#L274</a> github </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">/&lt;</span><span class="n">em</span><span class="o">&gt;*</span> <span class="n">Given</span> <span class="n">a</span> <span class="n">node</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="o">;</span><span class="n">s</span> <span class="n">string</span> <span class="n">representation</span><span class="o">,</span> <span class="k">return</span> <span class="n">a</span> <span class="n">reference</span> <span class="n">to</span> <span class="n">the</span> <span class="n">node</span>
</span><span class='line'> <span class="o">*</span> <span class="nd">@param</span> <span class="n">loc</span> <span class="n">string</span> <span class="n">location</span> <span class="n">of</span> <span class="n">the</span> <span class="n">form</span> <span class="o">/</span><span class="n">rack</span><span class="o">/</span><span class="n">node</span>
</span><span class='line'> <span class="o">*</span> <span class="nd">@return</span> <span class="kc">null</span> <span class="k">if</span> <span class="n">the</span> <span class="n">node</span> <span class="n">is</span> <span class="n">not</span> <span class="n">found</span> <span class="n">or</span> <span class="n">the</span> <span class="n">childnode</span> <span class="n">is</span> <span class="n">there</span> <span class="n">but</span>
</span><span class='line'> <span class="o">*</span> <span class="n">not</span> <span class="n">an</span> <span class="n">instance</span> <span class="n">of</span> <span class="o">{</span><span class="nd">@link</span> <span class="n">InnerNode</span><span class="o">}</span>
</span><span class='line'> <span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;/</span>
</span><span class='line'><span class="kd">private</span> <span class="n">Node</span> <span class="nf">getLoc</span><span class="o">(</span><span class="n">String</span> <span class="n">loc</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">loc</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">||</span> <span class="n">loc</span><span class="o">.</span><span class="na">length</span><span class="o">()</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="k">return</span> <span class="k">this</span><span class="o">;</span>         <span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span>
</span><span class='line'>  <span class="n">String</span><span class="o">[]</span> <span class="n">path</span> <span class="o">=</span> <span class="n">loc</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="n">PATH_SEPARATOR_STR</span><span class="o">,</span> <span class="mi">2</span><span class="o">);</span>
</span><span class='line'>  <span class="n">Node</span> <span class="n">childnode</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>  <span class="k">for</span><span class="o">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="o">;</span> <span class="n">i</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">children</span><span class="o">.</span><span class="na">size</span><span class="o">();</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">children</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">).</span><span class="na">getName</span><span class="o">().</span><span class="na">equals</span><span class="o">(</span><span class="n">path</span><span class="o">[</span><span class="mi">0</span><span class="o">]))</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">childnode</span> <span class="o">=</span> <span class="n">children</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">childnode</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="k">return</span> <span class="kc">null</span><span class="o">;</span> <span class="c1">// non-existing node</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">path</span><span class="o">.</span><span class="na">length</span> <span class="o">==</span> <span class="mi">1</span><span class="o">)</span> <span class="k">return</span> <span class="n">childnode</span><span class="o">;</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">childnode</span> <span class="k">instanceof</span> <span class="n">InnerNode</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="o">((</span><span class="n">InnerNode</span><span class="o">)</span><span class="n">childnode</span><span class="o">).</span><span class="na">getLoc</span><span class="o">(</span><span class="n">path</span><span class="o">[</span><span class="mi">1</span><span class="o">]);</span>
</span><span class='line'>  <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>这段逻辑的用意是通过集群网络拓扑结构中节点的字符串标识（如：/IDC/Rack/hostname）获取该节点的对象。实现方法是从拓扑结构中根节点开始逐层向下搜索，直到找到对应的目标节点，逻辑本身没有问题，但是在line286处应该正常break，实现时出现遗漏，其结果是多出一些不必要的时间开销，对于小集群可能影响不大，但是对于IO比较密集的大集群其实影响还是比较大，线下模拟~5K节点的集群拓扑结构，对于NetworkTopology.InnerNode#getLoc(String loc)本身，break可以提升一半的时间开销。</p>

<p>3、通过前面两个阶段仍然不能完全解决问题，只能继续追加日志，这里再次怀疑可能BlockManager.computeReplicationWorkForBlocks(List&lt;List<Block>> blocksToReplicate)在调用chooseRandom方法时耗时严重，所以在chooseRandom结束后增加了关键的几条日志：<br/>
<figure class='code'><figcaption><span>BlockManager.java <a href="https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1322">https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1322</a> github </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">block</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">rw</span><span class="o">.</span><span class="na">block</span><span class="o">);</span>
</span><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">priority</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">rw</span><span class="o">.</span><span class="na">priority</span><span class="o">);</span>
</span><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">srcNode</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">rw</span><span class="o">.</span><span class="na">srcNode</span><span class="o">);</span>
</span><span class='line'><span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">storagepolicyid</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">rw</span><span class="o">.</span><span class="na">bc</span><span class="o">.</span><span class="na">getStoragePolicyID</span><span class="o">());</span>
</span><span class='line'><span class="k">if</span> <span class="o">(</span><span class="n">rw</span><span class="o">.</span><span class="na">targets</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">||</span> <span class="n">rw</span><span class="o">.</span><span class="na">targets</span><span class="o">.</span><span class="na">length</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">targets</span> <span class="n">is</span> <span class="n">empty</span><span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;);</span>
</span><span class='line'><span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">targets</span><span class="o">.</span><span class="na">length</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">rw</span><span class="o">.</span><span class="na">targets</span><span class="o">.</span><span class="na">length</span><span class="o">);</span>
</span><span class='line'>  <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span> <span class="n">rw</span><span class="o">.</span><span class="na">targets</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">target</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">rw</span><span class="o">.</span><span class="na">targets</span><span class="o">[</span><span class="n">i</span><span class="o">]</span> <span class="o">+</span> <span class="o">&amp;</span><span class="n">ldquo</span><span class="o">;,</span> <span class="n">StorageType</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">rw</span><span class="o">.</span><span class="na">targets</span><span class="o">[</span><span class="n">i</span><span class="o">].</span><span class="na">getStorageType</span><span class="o">());</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="n">Iterator</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">iterator</span> <span class="o">=</span> <span class="n">excludedNodes</span><span class="o">.</span><span class="na">iterator</span><span class="o">();</span> <span class="n">iterator</span><span class="o">.</span><span class="na">hasNext</span><span class="o">();)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">DatanodeDescriptor</span> <span class="n">node</span> <span class="o">=</span> <span class="o">(</span><span class="n">DatanodeDescriptor</span><span class="o">)</span> <span class="n">iterator</span><span class="o">.</span><span class="na">next</span><span class="o">();</span>
</span><span class='line'>  <span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="nl">ReplicationMonitor:</span> <span class="n">exclude</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span> <span class="o">+</span> <span class="n">node</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>包括当前正在处理的Block，优先级（标识缺块的严重程度），源和目标节点集合；（遗漏了关键的信息：Block的Numbytes，后面后详细解释。）</p>

<p>通过这一步基本上能够收集到异常现场信息，同时也可确定异常时ReplicationMonitor的运行情况，从后续的日志也能说明这一点：<br/>
<figure class='code'><figcaption><span>namenode.log </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>2016-04-19 20:08:52,328 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 7 to reach 10 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
</span><span class='line'>2016-04-19 20:08:52,328 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 7 to reach 10 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
</span><span class='line'>2016-04-19 20:08:52,328 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 7 to reach 10 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable: unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
</span><span class='line'>2016-04-19 20:08:52,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: block = blk_8206926206_7139007477
</span><span class='line'>2016-04-19 20:08:52,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: priority = 2
</span><span class='line'>2016-04-19 20:08:52,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: srcNode = 10.16.<span class="nt">&lt;em&gt;</span>.<span class="nt">&lt;/em&gt;</span>:<span class="nt">&lt;em&gt;</span>
</span><span class='line'>2016-04-19 20:08:52,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: storagepolicyid = 0
</span><span class='line'>2016-04-19 20:08:52,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: targets is empty
</span><span class='line'>2016-04-19 20:08:52,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: exclude = 10.16.<span class="nt">&lt;/em&gt;</span>.<span class="nt">&lt;em&gt;</span>:<span class="nt">&lt;/em&gt;</span>
</span><span class='line'>2016-04-19 20:08:52,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: exclude = 10.16.<span class="nt">&lt;em&gt;</span>.<span class="nt">&lt;/em&gt;</span>:<span class="nt">&lt;em&gt;</span>
</span><span class='line'>2016-04-19 20:08:52,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: exclude = 10.16.<span class="nt">&lt;/em&gt;</span>.<span class="nt">&lt;em&gt;</span>:<span class="nt">&lt;/em&gt;</span>
</span><span class='line'>2016-04-19 20:08:52,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor: exclude = 10.16.<span class="nt">&lt;em&gt;</span>.<span class="nt">&lt;/em&gt;</span>:**
</span></code></pre></td></tr></table></div></figure></p>

<p>日志中容易看到当前发生异常时的场景：</p>

<p>（1）Block的副本数尝试从3调整的10；（异常时还有其他副本增加的请求）<br/>
（2）ReplicationMonitor尝试进行副本调整时失败，原因是遍历全集群后并没有找到一个合适的节点给该Block提供副本存储，日志显示全集群的节点均被加入到了exclude；</p>

<p>基本能够确定由于chooseRandom函数遍历了全集群导致处理某（些）Block耗时严重，类似情况累积会恶化这种问题；</p>

<p>到这里基本可以解释为什么几个关键数据结构（UnderReplicatedBlocks和PendingDeletionBlocks）的量持续增加，根本原因在于ReplicationMonitor在尝试对某个Block进行副本调整时，遍历全集群不能选出合适的节点，导致处理一个Block都会耗时严重，如果多个类似Block累积会滚雪球式使情况恶化，而且更加糟糕的是UnderReplicatedBlocks本质是一个优先级队列，如果正好这些Block的优先级较高，处理失败发生超时后还会回到原来的优先队列里，导致后续正常Block也会被阻塞，即使在超时时间范围内ReplicationMonitor可以正常工作，限于其本身的限流及周期（3s）运行机制，实际上可处理的规模非常小，而UnderReplicatedBlocks及PendingDeletionBlocks的生产者丝毫没有变慢，所以造成了数据源源不断的进入队列，但是消费非常缓慢。线上监控数据看到某次极端情况一度累积到1000K规模的UnderReplicatedBlocks，其实风险已经非常高了。</p>

<p>虽然从日志能够解释通UnderReplicatedBlocks和PendingDeletionBlocks持续升高了，但是仍然遗留了一个关键问题：为什么在副本调整时全集群遍历都没有选出合适的节点？</p>

<h3>2.3 暴力破解</h3>

<p>此前已经在社区找到类似问题反馈：
<a href="https://issues.apache.org/jira/browse/HDFS-8718">https://issues.apache.org/jira/browse/HDFS-8718</a>
但是很遗憾没看到解决方案；</p>

<p>尝试从各种可能和怀疑中解释前面留下的问题并在线下进行各种场景复现：<br/>
（1）线下模拟了~5000节点集群规模遍历的时间开销，基本能够反映线上的情况；<br/>
（2）构造负载严重不均衡时节点选择的场景，不能复现；<br/>
（3）异构存储实现逻辑可能造成的chooseRandom遍历全集群，尝试构造各种异构存储组合并，不能复现；<br/>
（4）并发进行删除和副本调整，没有复现；（后面详细介绍）</p>

<p>其实（2）和（3）的验证必要性不是很大，负载问题通过源码简单分析即可，异构存储线上并没有开启。复现结果是：没有结果。</p>

<p>不得已选择临时解决方案：在BlockManager.computeReplicationWorkForBlocks(List&lt;List<Block>> blocksToReplicate)的第二个阶段，针对需要调整副本的Block集合批量进行chooseTargets时加入时间判断，并设定了阈值，当超时发生时退出本轮目标选择逻辑，可以解决PendingDeletionBlocks长时间不能被处理到的问题，代价是牺牲少量处理UnderReplicatedBlocks的时间；上线后符合预期，PendingDeletionBlocks规模得到了有效控制，但是UnderReplicatedBlocks的问题依然存在。</p>

<h3>2.4 调整参数</h3>

<p>期间，我们从前面新增的日志里同时发现了一个有意思的现象，正常情况下workFound的值相对较高，但是一旦出现异常，开始严重下降。workFound标识的是ReplicationMonitor本轮可以调度出去的Block数，影响该值的三个关键参数如下：<br/>
<figure class='code'><figcaption><span>hdfs-site.xml <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a> apache </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>dfs.namenode.replication.work.multiplier.per.iteration<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>5<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>dfs.namenode.replication.max-streams<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>50<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;description&gt;</span>
</span><span class='line'>        The maximum number of outgoing replication streams a given node should have
</span><span class='line'>        at one time considering all but the highest priority replications needed.
</span><span class='line'>  <span class="nt">&lt;/description&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>dfs.namenode.replication.max-streams-hard-limit<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;description&gt;</span>
</span><span class='line'>        The maximum number of outgoing replication streams a given node should have
</span><span class='line'>        at one time.
</span><span class='line'>  <span class="nt">&lt;/description&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>为控制单DN并发数默认值为&lt;2,2,4>，此外可调度的Block数与集群规模正相关，正常情况其实完全满足运行需求，但是由于存在Block不符预期，所以造成workFound量会下降。<br/>
结合集群实际基础环境，尝试大幅提高并发度，设置为&lt;5,50,100>，提高ReplicationMonitor每一轮的处理效率。参数调整后，情况得到了明显改善。</p>

<p><em>说明：结合实际情况谨慎调整该参数，可能会给集群内的网络带来压力。</em></p>

<p>虽然通过一系列调整能够暂缓和改善线上情况，但是依然没有回答前面留下的疑问，也没有彻底解决问题。只有从头再来梳理流程。</p>

<h2>三、ReplicationMonitor工作流程</h2>

<p>ReplicationMonitor是NameNode内部线程，负责维护数据块的副本数稳定，包括清理无效块和对不符预期副本数的Block进行增删工作。ReplicationMonitor是周期运行线程，默认每3s执行一次，主要由两个关键函数组成：computeDatanodeWork和processPendingReplications（rescanPostponedMisreplicatedBlocks在NameNode启动/主从切换被调用，不包括在本次异常分析范围内）。<br/>
<figure class='code'><figcaption><span>BlockManager.java <a href="https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L3621">https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L3621</a> github </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">privateclass</span> <span class="n">ReplicationMonitor</span> <span class="kd">implements</span> <span class="n">Runnable</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="nd">@Override</span>
</span><span class='line'>  <span class="n">publicvoid</span> <span class="nf">run</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">while</span> <span class="o">(</span><span class="n">namesystem</span><span class="o">.</span><span class="na">isRunning</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// Process replication work only when active NN is out of safe mode.</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">namesystem</span><span class="o">.</span><span class="na">isPopulatingReplQueues</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">computeDatanodeWork</span><span class="o">();</span>
</span><span class='line'>          <span class="n">processPendingReplications</span><span class="o">();</span>
</span><span class='line'>          <span class="n">rescanPostponedMisreplicatedBlocks</span><span class="o">();</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="n">Thread</span><span class="o">.</span><span class="na">sleep</span><span class="o">(</span><span class="n">replicationRecheckInterval</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Throwable</span> <span class="n">t</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="o">&amp;</span><span class="n">hellip</span><span class="o">;&amp;</span><span class="n">hellip</span><span class="o">;</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><strong>computeDatanodeWork</strong><br/>
（1）从UnderReplicatedBlocks中取出给定阈值（默认为集群节点总数的2倍）数量范围内需要进行复制的Block集合；由于UnderReplicatedBlocks是一个优先级队列，所以每次一定是按照优先级从高到低获取；<br/>
（2）遍历选出的Block集合，对于每一个Block，根据当前副本分布及chooseTarget策略，选择合适的DataNode集合作为目标节点，准备副本复制；<br/>
（3）将Block进行副本复制的指令分发到NameNode里对应DatanodeDescriptor数据结构中，待该DataNode下次heartbeat过来后及时下发，同时将该Block从UnderReplicatedBlocks拿出来暂存到pendingReplications；<br/>
（4）DataNode接收到指令后把对应Block复制到目标节点，复制结束后，目标节点向NameNode汇报RECEIVED_BLOCK，此后便可以从pendingReplications中删除对应的Block；这里引入pendingReplications的目的是防止Block在复制过程中出现异常后超时，当在给定时间内（默认为5min）仍没有完成复制，需要将其从pendingReplications转移到timedOutItems集合中；超时检查的工作由PendingReplicationBlocks#PendingReplicationMonitor负责。<br/>
（5）将InvalidateBlocks中待删除的Blocks按照DataNode分组后取出分发到NameNode里对应DatanodeDescriptor数据结构中，同样待该DataNode的heartbeat过来后及时下发删除指令；</p>

<p><strong>processPendingReplications</strong><br/>
computeDatanodeWork步骤4出现超时后，将对应的Block从pendingReplications转移到timedOutItems后并没有其他处理逻辑，但是Block复制的事情还得继续，所以还需要将Block再拿回到UnderReplicatedBlocks后重复前面的工作；从timedOutItems拿回到UnderReplicatedBlocks的工作即由processPendingReplications来负责；</p>

<p>可以看出computeDatanodeWork，processPendingReplications和PendingReplicationMonitor组成了一个生产者消费者的环，下图可以说明这个过程。</p>

<div class=“pic” align="center" padding=“0”>
<img src="http://hexiaoqiao.github.io/images/monitor/replicationmonitor.png" alt="ReplicationMonitor相关数据流动图示" align="center"><br />
<label class=“pic_title” align="center">图3 ReplicationMonitor相关数据流动图示</label>
</div>


<p></p>

<p>ReplicationMonitor涉及到两个关键的数据结构：UnderReplicatedBlocks和InvalidateBlocks；这两个数据结构到底是什么，数据哪里来。</p>

<p>（1）UnderReplicatedBlocks：副本数不足的Block集合；<br/>
* 写数据完成时进行副本检查，副本不足Block；<br/>
* 用户调用setReplication增加副本；<br/>
* DataNode节点异常，其上的所有Block；</p>

<p>（2）InvalidateBlocks：无效Block集合；<br/>
* 文件删除操作；<br/>
* 用户调用setReplication降低副本；</p>

<p>可以简单理解副本调整和数据删除本质上是一个异步操作，当NameNode接收到客户端的setReplication或delete请求后，简单处理后即可返回，实际的工作是由ReplicationMonitor周期异步进行处理。</p>

<h2>四、根本原因</h2>

<p>继续追踪日志时，针对触发问题的Block检索了所有日志，发现另外一个现象，每次chooseTarget目标节点选择失败后，总会紧跟一条删除操作的日志：<br/>
<figure class='code'><figcaption><span>namenode.log </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>2016-04-19 09:23:08,453 INFO BlockStateChange: BLOCK<span class="nt">&lt;em&gt;</span> addToInvalidates: blk_8197702254_7129783384 10.16.<span class="nt">&lt;/em&gt;</span>.<span class="nt">&lt;em&gt;</span>:<span class="nt">&lt;/em&gt;</span> 10.16.<span class="nt">&lt;em&gt;</span>.<span class="nt">&lt;/em&gt;</span>:<span class="nt">&lt;em&gt;</span> 10.16.<span class="nt">&lt;/em&gt;</span>.*:**
</span></code></pre></td></tr></table></div></figure></p>

<p>审计日志中也能对照到同一时间点，对应的文件确实有删除操作。这个现象在多次异常复现时稳定发生。可以猜测与删除操作有关联。</p>

<p>删除操作的流程是先把目录树的节点删除，根据删除结果收集到的Block集合，删除每一个Block。</p>

<p>其中在逐个Block进行删除过程中，发现其逻辑有疑点，主要在block.setNumBytes(BlockCommand.NO_ACK)，其中NO_ACK=Long.MAX_VALUE，也即先将该Block的numbytes设置为最大值，再后续的操作。<br/>
<figure class='code'><figcaption><span>BlockManager.java <a href="https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L3378">https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L3378</a> github </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kt">void</span> <span class="nf">removeBlock</span><span class="o">(</span><span class="n">Block</span> <span class="n">block</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">assert</span> <span class="n">namesystem</span><span class="o">.</span><span class="na">hasWriteLock</span><span class="o">();</span>
</span><span class='line'>  <span class="c1">// No need to ACK blocks that are being removed entirely</span>
</span><span class='line'>  <span class="c1">// from the namespace, since the removal of the associated</span>
</span><span class='line'>  <span class="c1">// file already removes them from the block map below.</span>
</span><span class='line'>  <span class="n">block</span><span class="o">.</span><span class="na">setNumBytes</span><span class="o">(</span><span class="n">BlockCommand</span><span class="o">.</span><span class="na">NO_ACK</span><span class="o">);</span>
</span><span class='line'>  <span class="n">addToInvalidates</span><span class="o">(</span><span class="n">block</span><span class="o">);</span>
</span><span class='line'>  <span class="n">removeBlockFromMap</span><span class="o">(</span><span class="n">block</span><span class="o">);</span>
</span><span class='line'>  <span class="c1">// Remove the block from pendingReplications and neededReplications</span>
</span><span class='line'>  <span class="n">pendingReplications</span><span class="o">.</span><span class="na">remove</span><span class="o">(</span><span class="n">block</span><span class="o">);</span>
</span><span class='line'>  <span class="n">neededReplications</span><span class="o">.</span><span class="na">remove</span><span class="o">(</span><span class="n">block</span><span class="o">,</span> <span class="n">UnderReplicatedBlocks</span><span class="o">.</span><span class="na">LEVEL</span><span class="o">);</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">postponedMisreplicatedBlocks</span><span class="o">.</span><span class="na">remove</span><span class="o">(</span><span class="n">block</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">postponedMisreplicatedBlocksCount</span><span class="o">.</span><span class="na">decrementAndGet</span><span class="o">();</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>虽然对目录树的操作及removeBlock的操作均会持有全局写锁，但是很自然将Block的NumBytes设置成Long.MAX_VALUE的逻辑与chooseTarget遍历全集群仍不能选出合适节点的事实结合起来。</p>

<p>接下来自然是验证chooseTarget的处理逻辑：<br/>
<figure class='code'><figcaption><span>BlockManager.java <a href="https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1390">https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1390</a> github </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">&amp;</span><span class="n">hellip</span><span class="o">;&amp;</span><span class="n">hellip</span><span class="o">;</span>
</span><span class='line'><span class="o">}</span> <span class="k">finally</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">namesystem</span><span class="o">.</span><span class="na">writeUnlock</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="kd">final</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">excludedNodes</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashSet</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;();</span>
</span><span class='line'><span class="k">for</span><span class="o">(</span><span class="n">ReplicationWork</span> <span class="n">rw</span> <span class="o">:</span> <span class="n">work</span><span class="o">){</span>
</span><span class='line'>  <span class="c1">// Exclude all of the containing nodes from being targets.</span>
</span><span class='line'>  <span class="c1">// This list includes decommissioning or corrupt nodes.</span>
</span><span class='line'>  <span class="n">excludedNodes</span><span class="o">.</span><span class="na">clear</span><span class="o">();</span>
</span><span class='line'>  <span class="k">for</span> <span class="o">(</span><span class="n">DatanodeDescriptor</span> <span class="n">dn</span> <span class="o">:</span> <span class="n">rw</span><span class="o">.</span><span class="na">containingNodes</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">excludedNodes</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">dn</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="c1">// choose replication targets: NOT HOLDING THE GLOBAL LOCK</span>
</span><span class='line'>  <span class="c1">// It is costly to extract the filename for which chooseTargets is called,</span>
</span><span class='line'>  <span class="c1">// so for now we pass in the block collection itself.</span>
</span><span class='line'>  <span class="n">rw</span><span class="o">.</span><span class="na">chooseTargets</span><span class="o">(</span><span class="n">blockplacement</span><span class="o">,</span> <span class="n">storagePolicySuite</span><span class="o">,</span> <span class="n">excludedNodes</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="n">namesystem</span><span class="o">.</span><span class="na">writeLock</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>上面的实现可以看出，chooseTargets之前释放了全局锁，chooseTargets后重新申请到全局锁，唯独中间的chooseTargets在锁之外。至此，问题触发条件、场景等基本清楚：</p>

<p>（1）setReplication将文件的副本调大，此时会有一批属于该文件的Block进入UnderReplicatedBlocks等待ReplicationMonitor处理；<br/>
（2）ReplicationMonitor从UnderReplicatedBlocks中取出部分Block，并在前期根据处理逻辑初始化相关参数，将每个Block打包成ReplicationWork，取出的所有Block完成打包后组成ReplicationWork集合，这个过程持有全局锁；<br/>
（3）当步骤2释放完全局锁后，被删除请求的RPC抢到全局锁，恰好这次删除操作对应文件即是步骤（1）中的文件，此时Block的NumBytes被设置成Long.MAX_VALUE，并被从BlocksMap,pendingReplications及UnderReplicatedBlocks中删除，但是该Block对象的引用还被步骤（2）中的ReplicationWork集合持有，不会被JVM回收，不同的是ReplicationWork集合中对应Block的NumBytes已经被修改成Long.MAX_VALUE；<br/>
（4）ReplicationMonitor中computeReplicationWorkForBlocks继续进行chooseTarget时显然已经不可能在集群中选出合适的节点，即使遍历完整个集群，本质上还是由于块大小已经是Long.MAX_VALUE，不可能有节点能满足需求。</p>

<p>通过单元测试对该场景能够稳定复现。</p>

<h2>五、解决方式</h2>

<p>问题分析完后，解决办法其实比较简单：</p>

<p>（1）如果ReplicationMonitor遇到了Block的NumBytes=BlockCommand.NO_ACK，直接将该Block从UnderReplicatedBlocks中删除；<br/>
（2）如果chooseTarget时遇到了Block的NumBytes=BlockCommand.NO_ACK，直接返回空，无需再遍历整个集群节点；</p>

<p>至此彻底解决了线上隐藏将近了一个月的Bug。线上再没有出现该异常。详细Patch见：<a href="https://issues.apache.org/jira/browse/HDFS-10453">https://issues.apache.org/jira/browse/HDFS-10453</a> 。</p>

<h2>六、经验</h2>

<p>回头看追查的整个过程，有几点值得总结的经验：</p>

<p>1、日志经过多次才调整到位，中间遗漏了关键的信息（block.getNumBytes），如果开始及时收集到这个信息，可以省去很多时间，所以如果能够准确快速收集关键数据，问题已经解决一半；</p>

<p>2、场景复现时提高并发其实是可以复现的，当时仅利用小工具模拟简单的场景，没有在真实环境进行高并发复现，错过一次可以定位的机会，合理的假设怀疑和严谨的场景复现很重要；</p>

<p>3、虽然问题在线上存在了超过两周时间，但是并没有实际影响到集群正常服务，得益于中间合理可控的缓解手段。如果不能彻底解决可以尝试通过各种方法缓解或绕过问题值得借鉴，这种方法论随处可见，但是只有亲自趟过坑后才能印象深刻。</p>

<p>4、回过头再看整个问题，解决问题的思路没有问题，但追查过程其实存在一个严重Bug，不再展开详述。</p>
]]></content>
  </entry>
  
</feed>
